{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dcbcd6",
   "metadata": {},
   "source": [
    "# 🔬 OSSExtractor 表面合成参数提取工具 - 调试版本\n",
    "\n",
    "本notebook允许您逐步调试OSSExtractor的每个处理步骤，查看中间结果并优化参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29deb780",
   "metadata": {},
   "source": [
    "## 📦 导入必要的库和模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4e064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaowenyuan/miniconda3/envs/ossextractor/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有模块导入成功！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 添加模块路径\n",
    "sys.path.append('Text Parser')\n",
    "sys.path.append('Text Extraction')\n",
    "\n",
    "# 导入统一的处理模块\n",
    "from PDF_Unified_Processor import PDFUnifiedProcessor, save_contents_to_specific_folders\n",
    "from TXT_Processing import process_text_file_for_processing\n",
    "from Embedding_and_Similarity import process_text_file_for_embedding\n",
    "from Unified_Text_Processor import (\n",
    "    process_text_file_for_filter, process_text_file_for_abstract, process_text_file_for_summerized,\n",
    "    process_text_file_for_filter, process_text_file_for_abstract, \n",
    "    process_text_file_for_summerized\n",
    ")\n",
    "\n",
    "print(\"✅ 所有模块导入成功！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195375cc",
   "metadata": {},
   "source": [
    "## 🔄 处理流程说明\n",
    "\n",
    "**OSSExtractor的完整处理流程：**\n",
    "\n",
    "1. **PDF转文本** → 原始文本文件\n",
    "2. **文本预处理** → 段落分割和过滤\n",
    "3. **嵌入相似度筛选** → 从所有段落中选出最相关的N个段落\n",
    "4. **LLM内容过滤** → 从相似度筛选的段落中进一步筛选\n",
    "5. **抽象和总结** → 生成最终的结构化参数\n",
    "\n",
    "**段落数量变化示例：**\n",
    "- 原始文本: 100+ 段落\n",
    "- 预处理后: 50+ 段落  \n",
    "- 嵌入筛选后: 20 段落 (最相关的)\n",
    "- LLM过滤后: 10 段落 (最符合要求的)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aff02e",
   "metadata": {},
   "source": [
    "## 🔧 配置参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93a1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 将处理 1 个PDF文件:\n",
      "📁 主输出目录已设置为: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output\n",
      "----------------------------------------\n",
      "  1. 正在处理: 101021acsoprd7b00291.pdf\n",
      "     -> 将输出到: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 配置要处理的PDF文件\n",
    "pdf_files = [\n",
    "    '/Users/zhaowenyuan/Projects/FCPDExtractor/Data/papers/101021acsoprd7b00291.pdf',\n",
    "    # 如果有更多文件，可以加在这里\n",
    "    # '/Users/zhaowenyuan/Projects/FCPDExtractor/Data/papers/another_paper.pdf',\n",
    "]\n",
    "\n",
    "# 定义基础的数据目录\n",
    "base_data_dir = '/Users/zhaowenyuan/Projects/FCPDExtractor/Data'\n",
    "\n",
    "# 1. 在Data目录下，定义一个名为 'output' 的主输出文件夹路径\n",
    "main_output_dir = os.path.join(base_data_dir, 'output')\n",
    "\n",
    "# 2. 创建 'output' 文件夹 (如果它不存在的话)\n",
    "# exist_ok=True 表示如果文件夹已存在，则不会报错\n",
    "os.makedirs(main_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📄 将处理 {len(pdf_files)} 个PDF文件:\")\n",
    "print(f\"📁 主输出目录已设置为: {main_output_dir}\")\n",
    "print(\"-\" * 40) # 打印分割线\n",
    "\n",
    "# 遍历每一个要处理的PDF文件\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    \n",
    "    # 3. 从完整路径中获取PDF的文件名 (例如: 'd2cp03073j.pdf')\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    \n",
    "    # 4. 去掉.pdf扩展名，创建文件夹名 (例如: 'd2cp03073j')\n",
    "    folder_name = os.path.splitext(pdf_filename)[0]\n",
    "    \n",
    "    # 5. 拼接出这个PDF专属的输出文件夹的完整路径\n",
    "    specific_output_dir = os.path.join(main_output_dir, folder_name)\n",
    "    \n",
    "    # 6. 创建这个专属的文件夹\n",
    "    os.makedirs(specific_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"  {i}. 正在处理: {pdf_filename}\")\n",
    "    print(f\"     -> 将输出到: {specific_output_dir}\")\n",
    "\n",
    "    # --- 在这里接上你后续的处理逻辑 ---\n",
    "    # 例如，你之后所有保存文件的操作，都应该使用 `specific_output_dir` 作为路径\n",
    "    # processed_text_path = os.path.join(specific_output_dir, 'Processed_text.txt')\n",
    "    # with open(processed_text_path, 'w') as f:\n",
    "    #     f.write(\"这里是处理后的文本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4eb3a6",
   "metadata": {},
   "source": [
    "## 📄 步骤 1: PDF转文本处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a110213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 1/5: PDF转文本处理...\n",
      "==================================================\n",
      "✅ PDF转文本完成！生成了 1 个文本文件:\n",
      "  1. /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/101021acsoprd7b00291.txt\n",
      "     📊 行数: 731\n",
      "     📏 文件大小: 33175 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 1/5: PDF转文本处理...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 使用统一的PDF处理模块\n",
    "processor = PDFUnifiedProcessor()\n",
    "\n",
    "# 执行PDF转文本\n",
    "output_files = save_contents_to_specific_folders(pdf_files, main_output_dir)\n",
    "\n",
    "print(f\"✅ PDF转文本完成！生成了 {len(output_files)} 个文本文件:\")\n",
    "for i, file in enumerate(output_files, 1):\n",
    "    print(f\"  {i}. {file}\")\n",
    "    \n",
    "    # 显示文件大小和行数\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            print(f\"     📊 行数: {len(lines)}\")\n",
    "            print(f\"     📏 文件大小: {os.path.getsize(file)} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d163864",
   "metadata": {},
   "source": [
    "### 🔍 查看PDF转文本结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383ac1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看文件: 101021acsoprd7b00291.txt\n",
      "==================================================\n",
      "📊 总字符数: 32660\n",
      "📊 总行数: 731\n",
      "\n",
      "📄 前500个字符预览:\n",
      "------------------------------\n",
      "Process Development and Scale-up of the Continuous Flow Nitration\n",
      "of Triﬂuoromethoxybenzene\n",
      "Zhenghui Wen,†,‡ Fengjun Jiao,† Mei Yang,† Shuainan Zhao,†,‡ Feng Zhou,†,‡ and Guangwen Chen*,†\n",
      "†Dalian National Laboratory for Clean Energy, Dalian Institute of Chemical Physics, Chinese Academy of Sciences, Dalian 116023,\n",
      "China\n",
      "‡University of Chinese Academy of Sciences, Beijing 100049, China\n",
      "*\n",
      "S Supporting Information\n",
      "ABSTRACT: In this work, continuous ﬂow nitration of triﬂuoromethoxybenzene (TFMB) was...\n"
     ]
    }
   ],
   "source": [
    "# 选择第一个文件进行详细查看\n",
    "sample_file = output_files[0]\n",
    "print(f\"📖 查看文件: {os.path.basename(sample_file)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(f\"📊 总字符数: {len(content)}\")\n",
    "print(f\"📊 总行数: {len(content.splitlines())}\")\n",
    "print(\"\\n📄 前500个字符预览:\")\n",
    "print(\"-\" * 30)\n",
    "print(content[:500] + \"...\" if len(content) > 500 else content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90caea",
   "metadata": {},
   "source": [
    "### 🔍 结构化PDF解析（可选）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2aae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 结构化PDF解析（针对摘要和结论）\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: 101021acsoprd7b00291.pdf\n",
      "✅ other 章节: 8 个段落 -> /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/101021acsoprd7b00291_other.txt\n",
      "  ✅ other: 663 个段落\n",
      "\n",
      "🎉 结构化解析完成！\n"
     ]
    }
   ],
   "source": [
    "# 可选：使用结构化解析提取摘要和结论部分\n",
    "print(\"🔍 结构化PDF解析（针对摘要和结论）\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "structured_results = []\n",
    "\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(pdf_files)}: {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    # 使用统一处理器进行结构化解析\n",
    "    result = processor.process_pdf_comprehensive(pdf_path, main_output_dir, mode='structured')\n",
    "    structured_results.append(result)\n",
    "    \n",
    "    # 显示结果\n",
    "    for section, file_path in result.items():\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"  ✅ {section}: {len(lines)} 个段落\")\n",
    "\n",
    "print(f\"\\n🎉 结构化解析完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c6cf3",
   "metadata": {},
   "source": [
    "## 📝 步骤 2: 文本预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab72e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 2/5: 文本预处理...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: 101021acsoprd7b00291.txt\n",
      "  ✅ 预处理完成，过滤了 34 个段落\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/Processed_101021acsoprd7b00291.txt\n",
      "\n",
      "🎉 文本预处理完成！总共过滤了 34 个段落\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 2/5: 文本预处理...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_filtered_count = 0\n",
    "processed_files = []\n",
    "\n",
    "# 处理上一步生成的TXT文件，而不是PDF文件\n",
    "for i, txt_file in enumerate(output_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(output_files)}: {os.path.basename(txt_file)}\")\n",
    "    \n",
    "    # 执行文本预处理 - 处理TXT文件\n",
    "    processed_file_path, filtered_count = process_text_file_for_processing(txt_file)\n",
    "    processed_files.append(processed_file_path)\n",
    "    total_filtered_count += filtered_count\n",
    "    \n",
    "    print(f\"  ✅ 预处理完成，过滤了 {filtered_count} 个段落\")\n",
    "    print(f\"  📁 输出文件: {processed_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 文本预处理完成！总共过滤了 {total_filtered_count} 个段落\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab9779",
   "metadata": {},
   "source": [
    "### 🔍 查看预处理结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664887d",
   "metadata": {},
   "source": [
    "### 💡 LLM内容过滤说明\n",
    "\n",
    "**这一步的作用：**\n",
    "- 输入：嵌入相似度筛选出的段落（如20个段落）\n",
    "- 处理：使用Nous-Hermes-Llama2-13B模型判断每个段落是否真正与表面化学反应相关\n",
    "- 输出：进一步筛选的相关段落（如10个段落）\n",
    "\n",
    "**模型选择：**\n",
    "- 优先使用：Nous-Hermes-Llama2-13B-Instruct（更智能，性能更好）\n",
    "- 回退模型：nous-hermes-llama2-13b（稳定可靠）\n",
    "\n",
    "**为什么段落数会减少：**\n",
    "- 嵌入相似度只是基于关键词匹配\n",
    "- LLM过滤会进行更智能的内容理解\n",
    "- 最终保留真正相关的段落\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 分析文件: 101021acsoprd7b00291.txt\n",
      "==================================================\n",
      "📊 原始文件总行数: 731\n",
      "📊 空行数量: 43\n",
      "📊 分割后的段落数: 43\n",
      "📊 平均段落长度: 756.1 字符\n",
      "\n",
      "📄 前5个段落预览:\n",
      "------------------------------\n",
      "段落 1 (长度: 1160): Process Development and Scale-up of the Continuous Flow Nitration of Triﬂuoromethoxybenzene Zhenghui Wen,†,‡ Fengjun Jiao,† Mei Yang,† Shuainan Zhao,†...\n",
      "段落 2 (长度: 932): Triﬂuoromethoxy aniline is an important intermediate involved in the synthesis of a wide range of ﬁne chemicals, for example, pesticides,1 pharmaceuti...\n",
      "段落 3 (长度: 969): Therefore, it is very important and urgent to develop a new strategy based on process intensiﬁcation technology to improve the productivity and proces...\n",
      "段落 4 (长度: 926): Brocklehurst et al.8 used a commercially available continuous ﬂow reactor to perform the challenging nitration of 2-amino-4-bromobenzoic acid methyl e...\n",
      "段落 5 (长度: 1006): Therefore, the selectivity of m- NB and DNB should be controlled as low as possible to cut the cost of the separation. The objective was to minimize t...\n",
      "\n",
      "📊 总结:\n",
      "  - 原始文件行数: 43\n",
      "  - 预处理后段落数: 25\n",
      "  - 过滤掉的段落数: 18\n",
      "  - 保留比例: 58.1%\n"
     ]
    }
   ],
   "source": [
    "# 详细分析段落分割过程\n",
    "def analyze_paragraph_segmentation(txt_file):\n",
    "    print(f\"🔍 分析文件: {os.path.basename(txt_file)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"📊 原始文件总行数: {len(lines)}\")\n",
    "    \n",
    "    # 模拟段落分割过程\n",
    "    current_segment = []\n",
    "    segments = []\n",
    "    empty_lines_count = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():  # 非空行\n",
    "            current_segment.append(line.strip())\n",
    "        else:  # 空行\n",
    "            empty_lines_count += 1\n",
    "            if current_segment:  # 如果当前段落不为空\n",
    "                segments.append(' '.join(current_segment))\n",
    "                current_segment = []\n",
    "    \n",
    "    # 处理最后一个段落\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "    \n",
    "    print(f\"📊 空行数量: {empty_lines_count}\")\n",
    "    print(f\"📊 分割后的段落数: {len(segments)}\")\n",
    "    print(f\"📊 平均段落长度: {sum(len(seg) for seg in segments) / len(segments):.1f} 字符\")\n",
    "    \n",
    "    print(\"\\n📄 前5个段落预览:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, segment in enumerate(segments[:5]):\n",
    "        print(f\"段落 {i+1} (长度: {len(segment)}): {segment[:150]}...\" if len(segment) > 150 else f\"段落 {i+1} (长度: {len(segment)}): {segment}\")\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# 分析原始TXT文件的段落分割\n",
    "if output_files:\n",
    "    original_segments = analyze_paragraph_segmentation(output_files[0])\n",
    "    \n",
    "    print(f\"\\n📊 总结:\")\n",
    "    print(f\"  - 原始文件行数: {len(original_segments)}\")\n",
    "    print(f\"  - 预处理后段落数: 25\")\n",
    "    print(f\"  - 过滤掉的段落数: {len(original_segments) - 25}\")\n",
    "    print(f\"  - 保留比例: {25/len(original_segments)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b4595",
   "metadata": {},
   "source": [
    "## 🔍 步骤 3: 嵌入和相似度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d9b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 3/5: 嵌入和相似度计算...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Processed_101021acsoprd7b00291.txt\n",
      "  ✅ 嵌入和相似度计算完成\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/Embedding_101021acsoprd7b00291.txt\n",
      "\n",
      "🎉 嵌入和相似度计算完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 3/5: 嵌入和相似度计算...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "embedding_files = []\n",
    "\n",
    "# 使用上一步预处理后的文件\n",
    "for i, processed_file in enumerate(processed_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(processed_files)}: {os.path.basename(processed_file)}\")\n",
    "    \n",
    "    # 执行嵌入和相似度计算\n",
    "    embedding_file_path = process_text_file_for_embedding(processed_file)\n",
    "    embedding_files.append(embedding_file_path)\n",
    "    \n",
    "    print(f\"  ✅ 嵌入和相似度计算完成\")\n",
    "    print(f\"  📁 输出文件: {embedding_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 嵌入和相似度计算完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572a33b",
   "metadata": {},
   "source": [
    "### 🔍 查看嵌入结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0bc8271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看嵌入文件: Embedding_101021acsoprd7b00291.txt\n",
      "==================================================\n",
      "📊 嵌入相似度筛选后段落数: 20\n",
      "\n",
      "📄 相似度最高的段落预览:\n",
      "------------------------------\n",
      "段落 1: The general laboratory process developments were carried out in microchannel reactors. TFMB (7.57 M, ﬂow rate of 0.4−1.0 mL·min−1) and a solution of fuming nitric acid in concentrated sulfuric acid (2...\n",
      "段落 3: A faster ﬂow rate would result in a better mixing eﬀect, which can make the organic compound more evenly dispersed in the acid phase. Besides, a faster ﬂow rate would also decrease the residence time....\n"
     ]
    }
   ],
   "source": [
    "# 查看嵌入结果\n",
    "sample_embedding = embedding_files[0]\n",
    "print(f\"📖 查看嵌入文件: {os.path.basename(sample_embedding)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_embedding, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"📊 嵌入相似度筛选后段落数: {len(lines)}\")\n",
    "print(\"\\n📄 相似度最高的段落预览:\")\n",
    "print(\"-\" * 30)\n",
    "for i, line in enumerate(lines[:3]):\n",
    "    if line.strip():\n",
    "        print(f\"段落 {i+1}: {line[:200]}...\" if len(line) > 200 else f\"段落 {i+1}: {line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860a8f4",
   "metadata": {},
   "source": [
    "## 🤖 步骤 4: LLM内容过滤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af29236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 4/5: LLM内容过滤...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Embedding_101021acsoprd7b00291.txt\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "🔍 处理文件: Embedding_101021acsoprd7b00291.txt\n",
      "==================================================\n",
      "📊 原始段落数: 10\n",
      "\n",
      "🤖 步骤1: LLM内容过滤...\n",
      "...开始使用LLM进行段落分类...\n",
      "...分类完成，保留 4 个相关段落。\n",
      "✅ 过滤后段落数: 4\n",
      "  ✅ LLM内容过滤完成\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/Embedding_101021acsoprd7b00291_Filtered.txt\n",
      "\n",
      "🎉 LLM内容过滤完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 4/5: LLM内容过滤...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "filter_files = []\n",
    "\n",
    "for i, embedding_file in enumerate(embedding_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(embedding_files)}: {os.path.basename(embedding_file)}\")\n",
    "    \n",
    "    # 执行LLM内容过滤\n",
    "    filter_file_path = process_text_file_for_filter(embedding_file)\n",
    "    filter_files.append(filter_file_path)\n",
    "    \n",
    "    print(f\"  ✅ LLM内容过滤完成\")\n",
    "    print(f\"  📁 输出文件: {filter_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 LLM内容过滤完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522a3d6",
   "metadata": {},
   "source": [
    "### 🔍 查看LLM过滤结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4def142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看过滤文件: Embedding_101021acsoprd7b00291_Filtered.txt\n",
      "==================================================\n",
      "📊 LLM过滤后段落数: 8\n",
      "\n",
      "📄 过滤后的段落预览:\n",
      "------------------------------\n",
      "段落 1: The general laboratory process developments were carried out in microchannel reactors. TFMB (7.57 M, ﬂow rate of 0.4−1.0 mL·min−1) and a solution of fuming nitric acid in concentrated sulfuric acid (2...\n",
      "段落 3: The highest conversion (99.6%) of reactant was obtained in the microchannel coupled with tubular reactor system at the condition of N/S = 0.25, N/F = 1.1, φ = 97 wt %, T = 273 K, Qor = 0.4 mL·min−1, a...\n"
     ]
    }
   ],
   "source": [
    "# 查看LLM过滤结果\n",
    "sample_filter = filter_files[0]\n",
    "print(f\"📖 查看过滤文件: {os.path.basename(sample_filter)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_filter, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"📊 LLM过滤后段落数: {len(lines)}\")\n",
    "print(\"\\n📄 过滤后的段落预览:\")\n",
    "print(\"-\" * 30)\n",
    "for i, line in enumerate(lines[:3]):\n",
    "    if line.strip():\n",
    "        print(f\"段落 {i+1}: {line[:200]}...\" if len(line) > 200 else f\"段落 {i+1}: {line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88626257",
   "metadata": {},
   "source": [
    "## 📊 步骤 5: 抽象和总结\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11b502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 5/5: 抽象和总结...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Embedding_101021acsoprd7b00291_Filtered.txt\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "🔍 处理文件: Embedding_101021acsoprd7b00291_Filtered.txt\n",
      "==================================================\n",
      "📊 原始段落数: 4\n",
      "\n",
      "📝 步骤2: 文本抽象...\n",
      "Abstract 1/4:\n",
      "\n",
      "Abstract 2/4:\n",
      "\n",
      "Abstract 3/4:\n",
      "\n",
      "Abstract 4/4:\n",
      "\n",
      "  ✅ 抽象完成: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/Embedding_101021acsoprd7b00291_Filtered_Abstract.txt\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "🔍 处理文件: Embedding_101021acsoprd7b00291_Filtered.txt\n",
      "==================================================\n",
      "📊 原始段落数: 4\n",
      "\n",
      "📊 步骤3: 参数总结...\n",
      "Summarized 1/4:\n",
      "- Normalize all temperature values to °C for consistency in units across the JSON object.\n",
      "Summarized 2/4:\n",
      "- Do not add extra fields beyond the JSON schema or alter the data in any way other than to meet its structure.\n",
      "Summarized 3/4:\n",
      "- In the case of \"inner_diameter\" field, use lowercase (e.g., \"20 mm\").\n",
      "Summarized 4/4:\n",
      "- Do not modify the units of values in the original text; only normalize to standardized units (e.g., °C for temperature, min for residence time).\n",
      "  ✅ 总结完成: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/101021acsoprd7b00291/Embedding_101021acsoprd7b00291_Filtered_Summarized.txt\n",
      "\n",
      "🎉 抽象和总结完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 5/5: 抽象和总结...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "abstract_files = []\n",
    "summarized_files = []\n",
    "\n",
    "# 使用过滤后的文件进行抽象和总结\n",
    "for i, filter_file in enumerate(filter_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(filter_files)}: {os.path.basename(filter_file)}\")\n",
    "    \n",
    "    # 执行抽象\n",
    "    abstract_file_path = process_text_file_for_abstract(filter_file)\n",
    "    abstract_files.append(abstract_file_path)\n",
    "    print(f\"  ✅ 抽象完成: {abstract_file_path}\")\n",
    "    \n",
    "    # 执行总结\n",
    "    summerized_file_path = process_text_file_for_summerized(filter_file)\n",
    "    summarized_files.append(summerized_file_path)\n",
    "    print(f\"  ✅ 总结完成: {summerized_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 抽象和总结完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb377e",
   "metadata": {},
   "source": [
    "### 🔍 查看最终结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38340a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看最终总结: Embedding_101021acsoprd7b00291_Filtered_Summarized.txt\n",
      "==================================================\n",
      "📊 总结内容长度: 425 字符\n",
      "\n",
      "📄 总结内容预览:\n",
      "------------------------------\n",
      "- Normalize all temperature values to °C for consistency in units across the JSON object.\n",
      "\n",
      "- Do not add extra fields beyond the JSON schema or alter the data in any way other than to meet its structure.\n",
      "\n",
      "- In the case of \"inner_diameter\" field, use lowercase (e.g., \"20 mm\").\n",
      "\n",
      "- Do not modify the units of values in the original text; only normalize to standardized units (e.g., °C for temperature, min for residence time).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看最终总结结果\n",
    "sample_summarized = summarized_files[0]\n",
    "print(f\"📖 查看最终总结: {os.path.basename(sample_summarized)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_summarized, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(f\"📊 总结内容长度: {len(content)} 字符\")\n",
    "print(\"\\n📄 总结内容预览:\")\n",
    "print(\"-\" * 30)\n",
    "print(content[:1000] + \"...\" if len(content) > 1000 else content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901c5c",
   "metadata": {},
   "source": [
    "## 📊 处理结果统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a0cd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 OSSExtractor 处理结果统计\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>文件</th>\n",
       "      <th>原始PDF (MB)</th>\n",
       "      <th>预处理 (KB)</th>\n",
       "      <th>嵌入筛选 (KB)</th>\n",
       "      <th>LLM过滤 (KB)</th>\n",
       "      <th>最终总结 (KB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101021acsoprd7b00291.txt</td>\n",
       "      <td>0.03</td>\n",
       "      <td>26.96</td>\n",
       "      <td>8.26</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         文件  原始PDF (MB)  预处理 (KB)  嵌入筛选 (KB)  LLM过滤 (KB)  \\\n",
       "0  101021acsoprd7b00291.txt        0.03     26.96       8.26        3.62   \n",
       "\n",
       "   最终总结 (KB)  \n",
       "0       0.42  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 所有处理步骤完成！\n"
     ]
    }
   ],
   "source": [
    "# 统计处理结果\n",
    "print(\"📊 OSSExtractor 处理结果统计\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stats = []\n",
    "for i, file_path in enumerate(output_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # 统计各个步骤的文件大小\n",
    "    original_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0\n",
    "    \n",
    "    processed_file = processed_files[i] if i < len(processed_files) else None\n",
    "    processed_size = os.path.getsize(processed_file) if processed_file and os.path.exists(processed_file) else 0\n",
    "    \n",
    "    embedding_file = embedding_files[i] if i < len(embedding_files) else None\n",
    "    embedding_size = os.path.getsize(embedding_file) if embedding_file and os.path.exists(embedding_file) else 0\n",
    "    \n",
    "    filter_file = filter_files[i] if i < len(filter_files) else None\n",
    "    filter_size = os.path.getsize(filter_file) if filter_file and os.path.exists(filter_file) else 0\n",
    "    \n",
    "    summarized_file = summarized_files[i] if i < len(summarized_files) else None\n",
    "    summarized_size = os.path.getsize(summarized_file) if summarized_file and os.path.exists(summarized_file) else 0\n",
    "    \n",
    "    stats.append({\n",
    "        '文件': filename,\n",
    "        '原始PDF (MB)': round(original_size / 1024 / 1024, 2),\n",
    "        '预处理 (KB)': round(processed_size / 1024, 2),\n",
    "        '嵌入筛选 (KB)': round(embedding_size / 1024, 2),\n",
    "        'LLM过滤 (KB)': round(filter_size / 1024, 2),\n",
    "        '最终总结 (KB)': round(summarized_size / 1024, 2)\n",
    "    })\n",
    "\n",
    "# 显示统计表格\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "print(\"\\n🎉 所有处理步骤完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147de13a",
   "metadata": {},
   "source": [
    "## 🔧 调试和优化建议\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 调试和优化建议\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. 📊 检查嵌入相似度阈值\n",
    "   - 如果筛选的段落太少，可以降低相似度阈值\n",
    "   - 如果筛选的段落太多，可以提高相似度阈值\n",
    "\n",
    "2. 🤖 优化LLM提示词\n",
    "   - 在Filter.py中调整问题描述\n",
    "   - 在Summerized.py中调整参数提取提示词\n",
    "\n",
    "3. 📝 调整文本预处理\n",
    "   - 在TXT_Processing.py中修改过滤规则\n",
    "   - 调整段落分割策略\n",
    "\n",
    "4. 🔍 检查模型性能\n",
    "   - 观察LLM的响应质量\n",
    "   - 考虑调整模型参数（temp, top_p等）\n",
    "\n",
    "5. 📈 可视化处理流程\n",
    "   - 绘制各步骤的数据量变化\n",
    "   - 分析处理效率\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3912b46",
   "metadata": {},
   "source": [
    "## 📊 结果查看和分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b00104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 查看处理结果...\n",
      "==================================================\n",
      "\n",
      "📁 原始文本:\n",
      "  1. 101021acsoprd7b00291.txt (731 行)\n",
      "\n",
      "📁 预处理文本:\n",
      "  1. Processed_101021acsoprd7b00291.txt (68 行)\n",
      "\n",
      "📁 嵌入文件:\n",
      "  1. Embedding_101021acsoprd7b00291.txt (20 行)\n",
      "\n",
      "📁 过滤文件:\n",
      "  1. Embedding_101021acsoprd7b00291_Filtered.txt (8 行)\n",
      "\n",
      "📁 抽象文件:\n",
      "  1. Embedding_101021acsoprd7b00291_Filtered_Abstract.txt (8 行)\n",
      "\n",
      "📁 总结文件:\n",
      "  1. Embedding_101021acsoprd7b00291_Filtered_Summarized.txt (8 行)\n",
      "\n",
      "🎉 处理完成！共处理了 1 个PDF文件\n"
     ]
    }
   ],
   "source": [
    "# 查看最终结果\n",
    "print(\"📊 查看处理结果...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 显示所有生成的文件\n",
    "all_files = {\n",
    "    '原始文本': output_files,\n",
    "    '预处理文本': processed_files,\n",
    "    '嵌入文件': embedding_files,\n",
    "    '过滤文件': filter_files,\n",
    "    '抽象文件': abstract_files,\n",
    "    '总结文件': summarized_files\n",
    "}\n",
    "\n",
    "for category, files in all_files.items():\n",
    "    print(f\"\\n📁 {category}:\")\n",
    "    for i, file in enumerate(files, 1):\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"  {i}. {os.path.basename(file)} ({len(lines)} 行)\")\n",
    "        else:\n",
    "            print(f\"  {i}. {os.path.basename(file)} (文件不存在)\")\n",
    "\n",
    "print(f\"\\n🎉 处理完成！共处理了 {len(pdf_files)} 个PDF文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6f7ab",
   "metadata": {},
   "source": [
    "## 🔍 结果分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65ff5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 分析最终结果...\n",
      "==================================================\n",
      "📄 最终总结结果:\n",
      "\n",
      "文件 1: Embedding_101021acsoprd7b00291_Filtered_Summarized.txt\n",
      "内容预览:\n",
      "------------------------------\n",
      "- Normalize all temperature values to °C for consistency in units across the JSON object.\n",
      "\n",
      "- Do not add extra fields beyond the JSON schema or alter the data in any way other than to meet its structure.\n",
      "\n",
      "- In the case of \"inner_diameter\" field, use lowercase (e.g., \"20 mm\").\n",
      "\n",
      "- Do not modify the units of values in the original text; only normalize to standardized units (e.g., °C for temperature, min for residence time).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析最终结果\n",
    "print(\"🔍 分析最终结果...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 查看总结文件的内容\n",
    "if summarized_files:\n",
    "    print(\"📄 最终总结结果:\")\n",
    "    for i, file in enumerate(summarized_files, 1):\n",
    "        print(f\"\\n文件 {i}: {os.path.basename(file)}\")\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            print(\"内容预览:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "        else:\n",
    "            print(\"文件不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示处理统计信息\n",
    "print(\"\\n📊 处理统计信息:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "total_paragraphs_original = 0\n",
    "total_paragraphs_filtered = 0\n",
    "\n",
    "for i, (original, filtered) in enumerate(zip(processed_files, filter_files), 1):\n",
    "    if os.path.exists(original):\n",
    "        with open(original, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            original_lines = len(f.readlines())\n",
    "        total_paragraphs_original += original_lines\n",
    "    \n",
    "    if os.path.exists(filtered):\n",
    "        with open(filtered, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            filtered_lines = len(f.readlines())\n",
    "        total_paragraphs_filtered += filtered_lines\n",
    "        \n",
    "        print(f\"文件 {i}: {original_lines} → {filtered_lines} 段落 (保留率: {filtered_lines/original_lines*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n总计: {total_paragraphs_original} → {total_paragraphs_filtered} 段落\")\n",
    "print(f\"整体保留率: {total_paragraphs_filtered/total_paragraphs_original*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🎉 摘要结论专用处理完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ossextractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
