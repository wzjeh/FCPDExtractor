{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dcbcd6",
   "metadata": {},
   "source": [
    "# 🔬 OSSExtractor 表面合成参数提取工具 - 调试版本\n",
    "\n",
    "本notebook允许您逐步调试OSSExtractor的每个处理步骤，查看中间结果并优化参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29deb780",
   "metadata": {},
   "source": [
    "## 📦 导入必要的库和模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有模块导入成功！\n",
      "🔄 统一处理模块:\n",
      "  - PDF_Unified_Processor: 统一PDF处理 (PyMuPDF)\n",
      "  - Unified_Text_Processor: 统一文本处理 (LLM)\n",
      "  - 兼容原有接口，功能更强大\n",
      "  - 支持基础文本提取和结构化解析\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 添加模块路径\n",
    "sys.path.append('Text Parser')\n",
    "sys.path.append('Text Extraction')\n",
    "\n",
    "# 导入统一的处理模块\n",
    "from PDF_Unified_Processor import PDFUnifiedProcessor, save_contents_to_specific_folders\n",
    "from TXT_Processing import process_text_file_for_processing\n",
    "from Embedding_and_Similarity import process_text_file_for_embedding\n",
    "from Unified_Text_Processor import (\n",
    "    process_text_file_for_filter, process_text_file_for_abstract, process_text_file_for_summerized,\n",
    "    process_text_file_for_filter_meta_llama, process_text_file_for_abstract_meta_llama, \n",
    "    process_text_file_for_summerized_meta_llama\n",
    ")\n",
    "\n",
    "print(\"✅ 所有模块导入成功！\")\n",
    "print(\"🔄 统一处理模块:\")\n",
    "print(\"  - PDF_Unified_Processor: 统一PDF处理 (PyMuPDF)\")\n",
    "print(\"  - Unified_Text_Processor: 统一文本处理 (LLM)\")\n",
    "print(\"  - 兼容原有接口，功能更强大\")\n",
    "print(\"  - 支持基础文本提取和结构化解析\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195375cc",
   "metadata": {},
   "source": [
    "## 🔄 处理流程说明\n",
    "\n",
    "**OSSExtractor的完整处理流程：**\n",
    "\n",
    "1. **PDF转文本** → 原始文本文件\n",
    "2. **文本预处理** → 段落分割和过滤\n",
    "3. **嵌入相似度筛选** → 从所有段落中选出最相关的N个段落\n",
    "4. **LLM内容过滤** → 从相似度筛选的段落中进一步筛选\n",
    "5. **抽象和总结** → 生成最终的结构化参数\n",
    "\n",
    "**段落数量变化示例：**\n",
    "- 原始文本: 100+ 段落\n",
    "- 预处理后: 50+ 段落  \n",
    "- 嵌入筛选后: 20 段落 (最相关的)\n",
    "- LLM过滤后: 10 段落 (最符合要求的)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aff02e",
   "metadata": {},
   "source": [
    "## 🔧 配置参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a93a1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 将处理 1 个PDF文件:\n",
      "📁 主输出目录已设置为: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output\n",
      "----------------------------------------\n",
      "  1. 正在处理: on-surface-synthesis-of.pdf\n",
      "     -> 将输出到: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 配置要处理的PDF文件\n",
    "pdf_files = [\n",
    "    '/Users/zhaowenyuan/Projects/FCPDExtractor/Data/papers/on-surface-synthesis-of.pdf',\n",
    "    # 如果有更多文件，可以加在这里\n",
    "    # '/Users/zhaowenyuan/Projects/FCPDExtractor/Data/papers/another_paper.pdf',\n",
    "]\n",
    "\n",
    "# 定义基础的数据目录\n",
    "base_data_dir = '/Users/zhaowenyuan/Projects/FCPDExtractor/Data'\n",
    "\n",
    "# 1. 在Data目录下，定义一个名为 'output' 的主输出文件夹路径\n",
    "main_output_dir = os.path.join(base_data_dir, 'output')\n",
    "\n",
    "# 2. 创建 'output' 文件夹 (如果它不存在的话)\n",
    "# exist_ok=True 表示如果文件夹已存在，则不会报错\n",
    "os.makedirs(main_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📄 将处理 {len(pdf_files)} 个PDF文件:\")\n",
    "print(f\"📁 主输出目录已设置为: {main_output_dir}\")\n",
    "print(\"-\" * 40) # 打印分割线\n",
    "\n",
    "# 遍历每一个要处理的PDF文件\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    \n",
    "    # 3. 从完整路径中获取PDF的文件名 (例如: 'd2cp03073j.pdf')\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    \n",
    "    # 4. 去掉.pdf扩展名，创建文件夹名 (例如: 'd2cp03073j')\n",
    "    folder_name = os.path.splitext(pdf_filename)[0]\n",
    "    \n",
    "    # 5. 拼接出这个PDF专属的输出文件夹的完整路径\n",
    "    specific_output_dir = os.path.join(main_output_dir, folder_name)\n",
    "    \n",
    "    # 6. 创建这个专属的文件夹\n",
    "    os.makedirs(specific_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"  {i}. 正在处理: {pdf_filename}\")\n",
    "    print(f\"     -> 将输出到: {specific_output_dir}\")\n",
    "\n",
    "    # --- 在这里接上你后续的处理逻辑 ---\n",
    "    # 例如，你之后所有保存文件的操作，都应该使用 `specific_output_dir` 作为路径\n",
    "    # processed_text_path = os.path.join(specific_output_dir, 'Processed_text.txt')\n",
    "    # with open(processed_text_path, 'w') as f:\n",
    "    #     f.write(\"这里是处理后的文本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4eb3a6",
   "metadata": {},
   "source": [
    "## 📄 步骤 1: PDF转文本处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a110213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 1/5: PDF转文本处理...\n",
      "==================================================\n",
      "✅ PDF转文本完成！生成了 1 个文本文件:\n",
      "  1. /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/on-surface-synthesis-of.txt\n",
      "     📊 行数: 533\n",
      "     📏 文件大小: 26193 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 1/5: PDF转文本处理...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 使用统一的PDF处理模块\n",
    "processor = PDFUnifiedProcessor()\n",
    "\n",
    "# 执行PDF转文本\n",
    "output_files = save_contents_to_specific_folders(pdf_files, main_output_dir)\n",
    "\n",
    "print(f\"✅ PDF转文本完成！生成了 {len(output_files)} 个文本文件:\")\n",
    "for i, file in enumerate(output_files, 1):\n",
    "    print(f\"  {i}. {file}\")\n",
    "    \n",
    "    # 显示文件大小和行数\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            print(f\"     📊 行数: {len(lines)}\")\n",
    "            print(f\"     📏 文件大小: {os.path.getsize(file)} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d163864",
   "metadata": {},
   "source": [
    "### 🔍 查看PDF转文本结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "383ac1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看文件: on-surface-synthesis-of.txt\n",
      "==================================================\n",
      "📊 总字符数: 25875\n",
      "📊 总行数: 533\n",
      "\n",
      "📄 前500个字符预览:\n",
      "------------------------------\n",
      "On-Surface Synthesis of Oligo(indenoindene)\n",
      " Marco Di Giovannantonio,* Qiang Chen, José I. Urgel, Pascal Ruﬃeux, Carlo A. Pignedoli,\n",
      "Klaus Müllen,* Akimitsu Narita,* and Roman Fasel*\n",
      "Cite This: J. Am. Chem. Soc. 2020, 142, 12925−12929\n",
      "Read Online\n",
      "ACCESS\n",
      "Metrics & More\n",
      "Article Recommendations\n",
      "*\n",
      "sı\n",
      "Supporting Information\n",
      "ABSTRACT: Fully conjugated ladder polymers (CLP) possess unique optical and electronic properties and are considered\n",
      "promising materials for applications in (opto)electronic dev...\n"
     ]
    }
   ],
   "source": [
    "# 选择第一个文件进行详细查看\n",
    "sample_file = output_files[0]\n",
    "print(f\"📖 查看文件: {os.path.basename(sample_file)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(f\"📊 总字符数: {len(content)}\")\n",
    "print(f\"📊 总行数: {len(content.splitlines())}\")\n",
    "print(\"\\n📄 前500个字符预览:\")\n",
    "print(\"-\" * 30)\n",
    "print(content[:500] + \"...\" if len(content) > 500 else content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90caea",
   "metadata": {},
   "source": [
    "### 🔍 结构化PDF解析（可选）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a2aae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 结构化PDF解析（针对摘要和结论）\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: on-surface-synthesis-of.pdf\n",
      "✅ other 章节: 5 个段落 -> /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/on-surface-synthesis-of_other.txt\n",
      "  ✅ other: 489 个段落\n",
      "\n",
      "🎉 结构化解析完成！\n"
     ]
    }
   ],
   "source": [
    "# 可选：使用结构化解析提取摘要和结论部分\n",
    "print(\"🔍 结构化PDF解析（针对摘要和结论）\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "structured_results = []\n",
    "\n",
    "for i, pdf_path in enumerate(pdf_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(pdf_files)}: {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    # 使用统一处理器进行结构化解析\n",
    "    result = processor.process_pdf_comprehensive(pdf_path, main_output_dir, mode='structured')\n",
    "    structured_results.append(result)\n",
    "    \n",
    "    # 显示结果\n",
    "    for section, file_path in result.items():\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"  ✅ {section}: {len(lines)} 个段落\")\n",
    "\n",
    "print(f\"\\n🎉 结构化解析完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c6cf3",
   "metadata": {},
   "source": [
    "## 📝 步骤 2: 文本预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab72e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 2/5: 文本预处理...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: on-surface-synthesis-of.txt\n",
      "  ✅ 预处理完成，过滤了 19 个段落\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/Processed_on-surface-synthesis-of.txt\n",
      "\n",
      "🎉 文本预处理完成！总共过滤了 19 个段落\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 2/5: 文本预处理...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_filtered_count = 0\n",
    "processed_files = []\n",
    "\n",
    "# 处理上一步生成的TXT文件，而不是PDF文件\n",
    "for i, txt_file in enumerate(output_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(output_files)}: {os.path.basename(txt_file)}\")\n",
    "    \n",
    "    # 执行文本预处理 - 处理TXT文件\n",
    "    processed_file_path, filtered_count = process_text_file_for_processing(txt_file)\n",
    "    processed_files.append(processed_file_path)\n",
    "    total_filtered_count += filtered_count\n",
    "    \n",
    "    print(f\"  ✅ 预处理完成，过滤了 {filtered_count} 个段落\")\n",
    "    print(f\"  📁 输出文件: {processed_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 文本预处理完成！总共过滤了 {total_filtered_count} 个段落\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab9779",
   "metadata": {},
   "source": [
    "### 🔍 查看预处理结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664887d",
   "metadata": {},
   "source": [
    "### 💡 LLM内容过滤说明\n",
    "\n",
    "**这一步的作用：**\n",
    "- 输入：嵌入相似度筛选出的段落（如20个段落）\n",
    "- 处理：使用Meta-Llama-3.1-8B模型判断每个段落是否真正与表面化学反应相关\n",
    "- 输出：进一步筛选的相关段落（如10个段落）\n",
    "\n",
    "**模型选择：**\n",
    "- 优先使用：Meta-Llama-3.1-8B-Instruct（更智能，性能更好）\n",
    "- 回退模型：nous-hermes-llama2-13b（稳定可靠）\n",
    "\n",
    "**为什么段落数会减少：**\n",
    "- 嵌入相似度只是基于关键词匹配\n",
    "- LLM过滤会进行更智能的内容理解\n",
    "- 最终保留真正相关的段落\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40a707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 分析文件: on-surface-synthesis-of.txt\n",
      "==================================================\n",
      "📊 原始文件总行数: 533\n",
      "📊 空行数量: 34\n",
      "📊 分割后的段落数: 34\n",
      "📊 平均段落长度: 757.6 字符\n",
      "\n",
      "📄 前5个段落预览:\n",
      "------------------------------\n",
      "段落 1 (长度: 936): On-Surface Synthesis of Oligo(indenoindene) Marco Di Giovannantonio,* Qiang Chen, José I. Urgel, Pascal Ruﬃeux, Carlo A. Pignedoli, Klaus Müllen,* A...\n",
      "段落 2 (长度: 947): Achieving defect- free segments of oligo(indenoindene) oﬀers exclusive insight into this CLP and provides the basis to further synthetic approaches. C...\n",
      "段落 3 (长度: 896): The synthesis of PInIns was pioneered by Scherf and Müllen in 1992, but the ﬁnal dehydrogenation step did not proceed completely, and unambiguous str...\n",
      "段落 4 (长度: 943): Recently, we have established on-surface syntheses of indenoﬂuorene polymers by utilizing oxidative cyclization of methyl groups against phenylene rin...\n",
      "段落 5 (长度: 371): Reaction Pathway from 1 to p-OInIn 4 on Au(111) Communication pubs.acs.org/JACS © 2020 American Chemical Society 12925 https://dx.doi.org/10.1021/jacs...\n",
      "\n",
      "📊 总结:\n",
      "  - 原始文件行数: 34\n",
      "  - 预处理后段落数: 25\n",
      "  - 过滤掉的段落数: 9\n",
      "  - 保留比例: 73.5%\n"
     ]
    }
   ],
   "source": [
    "# 详细分析段落分割过程\n",
    "def analyze_paragraph_segmentation(txt_file):\n",
    "    print(f\"🔍 分析文件: {os.path.basename(txt_file)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"📊 原始文件总行数: {len(lines)}\")\n",
    "    \n",
    "    # 模拟段落分割过程\n",
    "    current_segment = []\n",
    "    segments = []\n",
    "    empty_lines_count = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():  # 非空行\n",
    "            current_segment.append(line.strip())\n",
    "        else:  # 空行\n",
    "            empty_lines_count += 1\n",
    "            if current_segment:  # 如果当前段落不为空\n",
    "                segments.append(' '.join(current_segment))\n",
    "                current_segment = []\n",
    "    \n",
    "    # 处理最后一个段落\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "    \n",
    "    print(f\"📊 空行数量: {empty_lines_count}\")\n",
    "    print(f\"📊 分割后的段落数: {len(segments)}\")\n",
    "    print(f\"📊 平均段落长度: {sum(len(seg) for seg in segments) / len(segments):.1f} 字符\")\n",
    "    \n",
    "    print(\"\\n📄 前5个段落预览:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, segment in enumerate(segments[:5]):\n",
    "        print(f\"段落 {i+1} (长度: {len(segment)}): {segment[:150]}...\" if len(segment) > 150 else f\"段落 {i+1} (长度: {len(segment)}): {segment}\")\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# 分析原始TXT文件的段落分割\n",
    "if output_files:\n",
    "    original_segments = analyze_paragraph_segmentation(output_files[0])\n",
    "    \n",
    "    print(f\"\\n📊 总结:\")\n",
    "    print(f\"  - 原始文件行数: {len(original_segments)}\")\n",
    "    print(f\"  - 预处理后段落数: 25\")\n",
    "    print(f\"  - 过滤掉的段落数: {len(original_segments) - 25}\")\n",
    "    print(f\"  - 保留比例: {25/len(original_segments)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b4595",
   "metadata": {},
   "source": [
    "## 🔍 步骤 3: 嵌入和相似度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d9b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 3/5: 嵌入和相似度计算...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Processed_on-surface-synthesis-of.txt\n",
      "  ✅ 嵌入和相似度计算完成\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/Embedding_on-surface-synthesis-of.txt\n",
      "\n",
      "🎉 嵌入和相似度计算完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 3/5: 嵌入和相似度计算...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "embedding_files = []\n",
    "\n",
    "# 使用上一步预处理后的文件\n",
    "for i, processed_file in enumerate(processed_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(processed_files)}: {os.path.basename(processed_file)}\")\n",
    "    \n",
    "    # 执行嵌入和相似度计算\n",
    "    embedding_file_path = process_text_file_for_embedding(processed_file)\n",
    "    embedding_files.append(embedding_file_path)\n",
    "    \n",
    "    print(f\"  ✅ 嵌入和相似度计算完成\")\n",
    "    print(f\"  📁 输出文件: {embedding_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 嵌入和相似度计算完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572a33b",
   "metadata": {},
   "source": [
    "### 🔍 查看嵌入结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0bc8271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看嵌入文件: Embedding_on-surface-synthesis-of.txt\n",
      "==================================================\n",
      "📊 嵌入相似度筛选后段落数: 20\n",
      "\n",
      "📄 相似度最高的段落预览:\n",
      "------------------------------\n",
      "段落 1: Recently, we have established on-surface syntheses of indenoﬂuorene polymers by utilizing oxidative cyclization of methyl groups against phenylene rings of polyphenylene backbones,25,26 an alternative...\n",
      "段落 3: The observed polymers were no longer packed into islands. This change can be attributed to the desorption of bromine atoms from the Au(111) surface, which is known to be promoted by the presence of at...\n"
     ]
    }
   ],
   "source": [
    "# 查看嵌入结果\n",
    "sample_embedding = embedding_files[0]\n",
    "print(f\"📖 查看嵌入文件: {os.path.basename(sample_embedding)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_embedding, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"📊 嵌入相似度筛选后段落数: {len(lines)}\")\n",
    "print(\"\\n📄 相似度最高的段落预览:\")\n",
    "print(\"-\" * 30)\n",
    "for i, line in enumerate(lines[:3]):\n",
    "    if line.strip():\n",
    "        print(f\"段落 {i+1}: {line[:200]}...\" if len(line) > 200 else f\"段落 {i+1}: {line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860a8f4",
   "metadata": {},
   "source": [
    "## 🤖 步骤 4: LLM内容过滤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9af29236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 4/5: LLM内容过滤...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Embedding_on-surface-synthesis-of.txt\n",
      "🔍 处理文件: Embedding_on-surface-synthesis-of.txt\n",
      "==================================================\n",
      "📊 原始段落数: 10\n",
      "\n",
      "🤖 步骤1: LLM内容过滤...\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "✅ 过滤后段落数: 4\n",
      "  ✅ LLM内容过滤完成\n",
      "  📁 输出文件: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/Embedding_on-surface-synthesis-of_Filtered.txt\n",
      "\n",
      "🎉 LLM内容过滤完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 4/5: LLM内容过滤...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "filter_files = []\n",
    "\n",
    "for i, embedding_file in enumerate(embedding_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(embedding_files)}: {os.path.basename(embedding_file)}\")\n",
    "    \n",
    "    # 执行LLM内容过滤\n",
    "    filter_file_path = process_text_file_for_filter_meta_llama(embedding_file)\n",
    "    filter_files.append(filter_file_path)\n",
    "    \n",
    "    print(f\"  ✅ LLM内容过滤完成\")\n",
    "    print(f\"  📁 输出文件: {filter_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 LLM内容过滤完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522a3d6",
   "metadata": {},
   "source": [
    "### 🔍 查看LLM过滤结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4def142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看过滤文件: Embedding_on-surface-synthesis-of_Filtered.txt\n",
      "==================================================\n",
      "📊 LLM过滤后段落数: 8\n",
      "\n",
      "📄 过滤后的段落预览:\n",
      "------------------------------\n",
      "段落 1: Recently, we have established on-surface syntheses of indenoﬂuorene polymers by utilizing oxidative cyclization of methyl groups against phenylene rings of polyphenylene backbones,25,26 an alternative...\n",
      "段落 3: The observed polymers were no longer packed into islands. This change can be attributed to the desorption of bromine atoms from the Au(111) surface, which is known to be promoted by the presence of at...\n"
     ]
    }
   ],
   "source": [
    "# 查看LLM过滤结果\n",
    "sample_filter = filter_files[0]\n",
    "print(f\"📖 查看过滤文件: {os.path.basename(sample_filter)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_filter, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(f\"📊 LLM过滤后段落数: {len(lines)}\")\n",
    "print(\"\\n📄 过滤后的段落预览:\")\n",
    "print(\"-\" * 30)\n",
    "for i, line in enumerate(lines[:3]):\n",
    "    if line.strip():\n",
    "        print(f\"段落 {i+1}: {line[:200]}...\" if len(line) > 200 else f\"段落 {i+1}: {line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88626257",
   "metadata": {},
   "source": [
    "## 📊 步骤 5: 抽象和总结\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a11b502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 步骤 5/5: 抽象和总结...\n",
      "==================================================\n",
      "\n",
      "📄 处理文件 1/1: Embedding_on-surface-synthesis-of_Filtered.txt\n",
      "🔍 处理文件: Embedding_on-surface-synthesis-of_Filtered.txt\n",
      "==================================================\n",
      "📊 原始段落数: 4\n",
      "\n",
      "📝 步骤2: 文本抽象...\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "Abstract 1/4:\n",
      " The resulting para-type oligo(indenoindene) (p-OInIn, 4) was characterized by means of low-temperature STM/STS and noncontact atomic force microscopy (nc-AFM). On the basis of our previous study on dihydroindenofluorene (DHIN),28 we propose that this reaction sequence is also applicable to other molecular systems with different substituents or structural features. In conclusion, these results provide an experimental demonstration for on-surface synthesis of para-type oligo(indenoindene) by thermally activated reactions, and offer a novel approach towards the precise control of molecular structures in surface chemistry. 24 POLYMERS ACCEPTED MANUSCRIPT\n",
      "Abstract 2/4:\n",
      "\n",
      "In this study, we have investigated various chemical reactions that occur at a temperature of 180 °C for Au(111) surfaces functionalized with bromine atoms. These reactions include desorption processes where hydrogen is present and oxidative cyclization of methyl groups on the polymer backbones.\n",
      "Abstract 3/4:\n",
      "Reaction Pathway from 1 to p-OInIn 4 on Au(111)\n",
      "Abstract 4/4:\n",
      "\n",
      " The experimental approach employed to investigate the adsorption and activation of CLP precursor molecules involved low-energy electron diffraction (LEED), scanning tunneling microscopy (STM), X-ray photoelectron spectroscopy (XPS) and density functional theory calculations (DFT).\n",
      "  ✅ 抽象完成: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/Embedding_on-surface-synthesis-of_Filtered_Abstract.txt\n",
      "🔍 处理文件: Embedding_on-surface-synthesis-of_Filtered.txt\n",
      "==================================================\n",
      "📊 原始段落数: 4\n",
      "\n",
      "📊 步骤3: 参数总结...\n",
      "🔍 尝试加载模型，路径: /Users/zhaowenyuan/Projects/FCPDExtractor/models\n",
      "✅ 成功加载 nous-hermes-llama2-13b.Q4_0.gguf 模型\n",
      "Summarized 1/4:\n",
      " -------------------------------- | ----------| -----------| --------| -----------| --------------- | dibromo-trimethyl-p-terphenyl (1)| Au(111)     | N/A         | p-OInIn 4| oligoindigoindene | monolayer, extended\n",
      "Summarized 2/4:\n",
      "\n",
      "Precursor   | Azobenzene-4-carboxylic acid N-hydroxysuccinimide ester | Au(111) surface | N/A          | N/A           | N/A           |\n",
      "Substrate  |                                |               |             |              |             |\n",
      "Temperature| N/A                            | 360 °C         | Oxidative cyclization products (e.g., azobenzene-4,4'-diyl bis(2-hydroxyethyl)succinate radicals) | 1D          |\n",
      "Product    | Azobenzene-4,4'-diyl bis(2-hydroxyethyl)succinate radicals                           |               |             |              | N/A           |\n",
      "Dimensions| 1D (azobenzene backbone)     |               |             |              | 1D          |\n",
      "Summarized 3/4:\n",
      " AU(111) - 4f - InCl + H2 + Heat 298K N/A OInInN+N-OInIn, (p-OH)(2+)-N-OInIn ClO3-, NH3 |\n",
      "Summarized 4/4:\n",
      "\n",
      "Precursor    substrate      temperature     product                             dimensions\n",
      "1               Au(111)       RT                         Oligo-p-phenylene (2,3,4...)  Ladder Polymer (3D)\n",
      "  ✅ 总结完成: /Users/zhaowenyuan/Projects/FCPDExtractor/Data/output/on-surface-synthesis-of/Embedding_on-surface-synthesis-of_Filtered_Summarized.txt\n",
      "\n",
      "🎉 抽象和总结完成！\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 步骤 5/5: 抽象和总结...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "abstract_files = []\n",
    "summarized_files = []\n",
    "\n",
    "# 使用过滤后的文件进行抽象和总结\n",
    "for i, filter_file in enumerate(filter_files, 1):\n",
    "    print(f\"\\n📄 处理文件 {i}/{len(filter_files)}: {os.path.basename(filter_file)}\")\n",
    "    \n",
    "    # 执行抽象\n",
    "    abstract_file_path = process_text_file_for_abstract_meta_llama(filter_file)\n",
    "    abstract_files.append(abstract_file_path)\n",
    "    print(f\"  ✅ 抽象完成: {abstract_file_path}\")\n",
    "    \n",
    "    # 执行总结\n",
    "    summerized_file_path = process_text_file_for_summerized_meta_llama(filter_file)\n",
    "    summarized_files.append(summerized_file_path)\n",
    "    print(f\"  ✅ 总结完成: {summerized_file_path}\")\n",
    "\n",
    "print(f\"\\n🎉 抽象和总结完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb377e",
   "metadata": {},
   "source": [
    "### 🔍 查看最终结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38340a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 查看最终总结: Embedding_on-surface-synthesis-of_Filtered_Summarized.txt\n",
      "==================================================\n",
      "📊 总结内容长度: 1192 字符\n",
      "\n",
      "📄 总结内容预览:\n",
      "------------------------------\n",
      " -------------------------------- | ----------| -----------| --------| -----------| --------------- | dibromo-trimethyl-p-terphenyl (1)| Au(111)     | N/A         | p-OInIn 4| oligoindigoindene | monolayer, extended\n",
      "\n",
      "\n",
      "Precursor   | Azobenzene-4-carboxylic acid N-hydroxysuccinimide ester | Au(111) surface | N/A          | N/A           | N/A           |\n",
      "Substrate  |                                |               |             |              |             |\n",
      "Temperature| N/A                            | 360 °C         | Oxidative cyclization products (e.g., azobenzene-4,4'-diyl bis(2-hydroxyethyl)succinate radicals) | 1D          |\n",
      "Product    | Azobenzene-4,4'-diyl bis(2-hydroxyethyl)succinate radicals                           |               |             |              | N/A           |\n",
      "Dimensions| 1D (azobenzene backbone)     |               |             |              | 1D          |\n",
      "\n",
      " AU(111) - 4f - InCl + H2 + Heat 298K N/A OInInN+N-OInIn, (p-OH)(2+)-N-OInIn ClO3-, NH3 |\n",
      "\n",
      "\n",
      "Precurs...\n"
     ]
    }
   ],
   "source": [
    "# 查看最终总结结果\n",
    "sample_summarized = summarized_files[0]\n",
    "print(f\"📖 查看最终总结: {os.path.basename(sample_summarized)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with open(sample_summarized, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(f\"📊 总结内容长度: {len(content)} 字符\")\n",
    "print(\"\\n📄 总结内容预览:\")\n",
    "print(\"-\" * 30)\n",
    "print(content[:1000] + \"...\" if len(content) > 1000 else content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901c5c",
   "metadata": {},
   "source": [
    "## 📊 处理结果统计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0cd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 OSSExtractor 处理结果统计\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>文件</th>\n",
       "      <th>原始PDF (MB)</th>\n",
       "      <th>预处理 (KB)</th>\n",
       "      <th>嵌入筛选 (KB)</th>\n",
       "      <th>LLM过滤 (KB)</th>\n",
       "      <th>最终总结 (KB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>on-surface-synthesis-of.txt</td>\n",
       "      <td>0.02</td>\n",
       "      <td>14.91</td>\n",
       "      <td>7.58</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            文件  原始PDF (MB)  预处理 (KB)  嵌入筛选 (KB)  LLM过滤 (KB)  \\\n",
       "0  on-surface-synthesis-of.txt        0.02     14.91       7.58        3.16   \n",
       "\n",
       "   最终总结 (KB)  \n",
       "0       1.17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 所有处理步骤完成！\n"
     ]
    }
   ],
   "source": [
    "# 统计处理结果\n",
    "print(\"📊 OSSExtractor 处理结果统计\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stats = []\n",
    "for i, file_path in enumerate(output_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # 统计各个步骤的文件大小\n",
    "    original_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0\n",
    "    \n",
    "    processed_file = processed_files[i] if i < len(processed_files) else None\n",
    "    processed_size = os.path.getsize(processed_file) if processed_file and os.path.exists(processed_file) else 0\n",
    "    \n",
    "    embedding_file = embedding_files[i] if i < len(embedding_files) else None\n",
    "    embedding_size = os.path.getsize(embedding_file) if embedding_file and os.path.exists(embedding_file) else 0\n",
    "    \n",
    "    filter_file = filter_files[i] if i < len(filter_files) else None\n",
    "    filter_size = os.path.getsize(filter_file) if filter_file and os.path.exists(filter_file) else 0\n",
    "    \n",
    "    summarized_file = summarized_files[i] if i < len(summarized_files) else None\n",
    "    summarized_size = os.path.getsize(summarized_file) if summarized_file and os.path.exists(summarized_file) else 0\n",
    "    \n",
    "    stats.append({\n",
    "        '文件': filename,\n",
    "        '原始PDF (MB)': round(original_size / 1024 / 1024, 2),\n",
    "        '预处理 (KB)': round(processed_size / 1024, 2),\n",
    "        '嵌入筛选 (KB)': round(embedding_size / 1024, 2),\n",
    "        'LLM过滤 (KB)': round(filter_size / 1024, 2),\n",
    "        '最终总结 (KB)': round(summarized_size / 1024, 2)\n",
    "    })\n",
    "\n",
    "# 显示统计表格\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "print(\"\\n🎉 所有处理步骤完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147de13a",
   "metadata": {},
   "source": [
    "## 🔧 调试和优化建议\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d08b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 调试和优化建议\n",
      "==================================================\n",
      "\n",
      "1. 📊 检查嵌入相似度阈值\n",
      "   - 如果筛选的段落太少，可以降低相似度阈值\n",
      "   - 如果筛选的段落太多，可以提高相似度阈值\n",
      "\n",
      "2. 🤖 优化LLM提示词\n",
      "   - 在Filter.py中调整问题描述\n",
      "   - 在Summerized.py中调整参数提取提示词\n",
      "\n",
      "3. 📝 调整文本预处理\n",
      "   - 在TXT_Processing.py中修改过滤规则\n",
      "   - 调整段落分割策略\n",
      "\n",
      "4. 🔍 检查模型性能\n",
      "   - 观察LLM的响应质量\n",
      "   - 考虑调整模型参数（temp, top_p等）\n",
      "\n",
      "5. 📈 可视化处理流程\n",
      "   - 绘制各步骤的数据量变化\n",
      "   - 分析处理效率\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"🔧 调试和优化建议\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. 📊 检查嵌入相似度阈值\n",
    "   - 如果筛选的段落太少，可以降低相似度阈值\n",
    "   - 如果筛选的段落太多，可以提高相似度阈值\n",
    "\n",
    "2. 🤖 优化LLM提示词\n",
    "   - 在Filter.py中调整问题描述\n",
    "   - 在Summerized.py中调整参数提取提示词\n",
    "\n",
    "3. 📝 调整文本预处理\n",
    "   - 在TXT_Processing.py中修改过滤规则\n",
    "   - 调整段落分割策略\n",
    "\n",
    "4. 🔍 检查模型性能\n",
    "   - 观察LLM的响应质量\n",
    "   - 考虑调整模型参数（temp, top_p等）\n",
    "\n",
    "5. 📈 可视化处理流程\n",
    "   - 绘制各步骤的数据量变化\n",
    "   - 分析处理效率\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3912b46",
   "metadata": {},
   "source": [
    "## 📊 结果查看和分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b00104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 查看处理结果...\n",
      "==================================================\n",
      "\n",
      "📁 原始文本:\n",
      "  1. on-surface-synthesis-of.txt (533 行)\n",
      "\n",
      "📁 预处理文本:\n",
      "  1. Processed_on-surface-synthesis-of.txt (38 行)\n",
      "\n",
      "📁 嵌入文件:\n",
      "  1. Embedding_on-surface-synthesis-of.txt (20 行)\n",
      "\n",
      "📁 过滤文件:\n",
      "  1. Embedding_on-surface-synthesis-of_Filtered.txt (8 行)\n",
      "\n",
      "📁 抽象文件:\n",
      "  1. Embedding_on-surface-synthesis-of_Filtered_Abstract.txt (10 行)\n",
      "\n",
      "📁 总结文件:\n",
      "  1. Embedding_on-surface-synthesis-of_Filtered_Summarized.txt (15 行)\n",
      "\n",
      "🎉 处理完成！共处理了 1 个PDF文件\n"
     ]
    }
   ],
   "source": [
    "# 查看最终结果\n",
    "print(\"📊 查看处理结果...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 显示所有生成的文件\n",
    "all_files = {\n",
    "    '原始文本': output_files,\n",
    "    '预处理文本': processed_files,\n",
    "    '嵌入文件': embedding_files,\n",
    "    '过滤文件': filter_files,\n",
    "    '抽象文件': abstract_files,\n",
    "    '总结文件': summarized_files\n",
    "}\n",
    "\n",
    "for category, files in all_files.items():\n",
    "    print(f\"\\n📁 {category}:\")\n",
    "    for i, file in enumerate(files, 1):\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"  {i}. {os.path.basename(file)} ({len(lines)} 行)\")\n",
    "        else:\n",
    "            print(f\"  {i}. {os.path.basename(file)} (文件不存在)\")\n",
    "\n",
    "print(f\"\\n🎉 处理完成！共处理了 {len(pdf_files)} 个PDF文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6f7ab",
   "metadata": {},
   "source": [
    "## 🔍 结果分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e65ff5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 分析最终结果...\n",
      "==================================================\n",
      "📄 最终总结结果:\n",
      "\n",
      "文件 1: Embedding_on-surface-synthesis-of_Filtered_Summarized.txt\n",
      "内容预览:\n",
      "------------------------------\n",
      " -------------------------------- | ----------| -----------| --------| -----------| --------------- | dibromo-trimethyl-p-terphenyl (1)| Au(111)     | N/A         | p-OInIn 4| oligoindigoindene | monolayer, extended\n",
      "\n",
      "\n",
      "Precursor   | Azobenzene-4-carboxylic acid N-hydroxysuccinimide ester | Au(111) surface | N/A          | N/A           | N/A           |\n",
      "Substrate  |                                |               |             |              |             |\n",
      "Temperature| N/A                        ...\n"
     ]
    }
   ],
   "source": [
    "# 分析最终结果\n",
    "print(\"🔍 分析最终结果...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 查看总结文件的内容\n",
    "if summarized_files:\n",
    "    print(\"📄 最终总结结果:\")\n",
    "    for i, file in enumerate(summarized_files, 1):\n",
    "        print(f\"\\n文件 {i}: {os.path.basename(file)}\")\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            print(\"内容预览:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "        else:\n",
    "            print(\"文件不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "712c2321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 处理统计信息:\n",
      "==============================\n",
      "文件 1: 38 → 8 段落 (保留率: 21.1%)\n",
      "\n",
      "总计: 38 → 8 段落\n",
      "整体保留率: 21.1%\n",
      "\n",
      "🎉 摘要结论专用处理完成！\n"
     ]
    }
   ],
   "source": [
    "# 显示处理统计信息\n",
    "print(\"\\n📊 处理统计信息:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "total_paragraphs_original = 0\n",
    "total_paragraphs_filtered = 0\n",
    "\n",
    "for i, (original, filtered) in enumerate(zip(processed_files, filter_files), 1):\n",
    "    if os.path.exists(original):\n",
    "        with open(original, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            original_lines = len(f.readlines())\n",
    "        total_paragraphs_original += original_lines\n",
    "    \n",
    "    if os.path.exists(filtered):\n",
    "        with open(filtered, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            filtered_lines = len(f.readlines())\n",
    "        total_paragraphs_filtered += filtered_lines\n",
    "        \n",
    "        print(f\"文件 {i}: {original_lines} → {filtered_lines} 段落 (保留率: {filtered_lines/original_lines*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n总计: {total_paragraphs_original} → {total_paragraphs_filtered} 段落\")\n",
    "print(f\"整体保留率: {total_paragraphs_filtered/total_paragraphs_original*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🎉 摘要结论专用处理完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ossextractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
