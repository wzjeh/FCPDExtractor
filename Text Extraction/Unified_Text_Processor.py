from gpt4all import GPT4All
import pandas as pd
import os
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class UnifiedTextProcessor:
    """
    ç»Ÿä¸€çš„æ–‡æœ¬å¤„ç†ç±»ï¼Œæ•´åˆæ‰€æœ‰æ–‡æœ¬å¤„ç†åŠŸèƒ½
    """
    
    def __init__(self, model_name='nous-hermes-llama2-13b.Q4_0.gguf', model_path='models/'):
        self.model_name = model_name
        self.model_path = model_path
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        
    def load_llm_model(self):
        """
        åŠ è½½LLMæ¨¡å‹ï¼Œæ”¯æŒå›é€€æœºåˆ¶
        """
        # è·å–ç»å¯¹è·¯å¾„
        abs_model_path = os.path.abspath(self.model_path)
        print(f"ğŸ” å°è¯•åŠ è½½æ¨¡å‹ï¼Œè·¯å¾„: {abs_model_path}")
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æŒ‡å®šæ¨¡å‹
            model = GPT4All(self.model_name, model_path=abs_model_path, allow_download=False)
            print(f"âœ… æˆåŠŸåŠ è½½ {self.model_name} æ¨¡å‹")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½ {self.model_name} å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„...")
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
                model = GPT4All(self.model_name, allow_download=False)
                print(f"âœ… æˆåŠŸåŠ è½½ {self.model_name} æ¨¡å‹ (é»˜è®¤è·¯å¾„)")
                return model
            except Exception as e2:
                print(f"âŒ é»˜è®¤è·¯å¾„ä¹Ÿå¤±è´¥: {e2}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨Meta-Llama-3.1-8Bæ¨¡å‹...")
                try:
                    # å°è¯•Meta-Llamaæ¨¡å‹
                    model = GPT4All('Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', model_path=abs_model_path, allow_download=False)
                    print("âœ… æˆåŠŸåŠ è½½ Meta-Llama-3.1-8B æ¨¡å‹ (å¤‡é€‰)")
                    return model
                except Exception as e3:
                    print(f"âŒ Meta-Llamaæ¨¡å‹ä¹Ÿå¤±è´¥: {e3}")
                    print("ğŸ”„ æœ€åå›é€€åˆ° nous-hermes-llama2-13b æ¨¡å‹...")
                    try:
                        # æœ€åå›é€€åˆ°nous-hermesæ¨¡å‹
                        model = GPT4All('nous-hermes-llama2-13b.Q4_0.gguf', model_path=abs_model_path, allow_download=False)
                        print("âœ… æˆåŠŸåŠ è½½ nous-hermes-llama2-13b æ¨¡å‹ (æœ€ç»ˆå›é€€)")
                        return model
                    except Exception as e4:
                        print(f"âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å°è¯•éƒ½å¤±è´¥: {e4}")
                        raise e4
    
    def filter_content_with_llm(self, df):
        """
        ä½¿ç”¨LLMè¿‡æ»¤å†…å®¹ï¼ˆæ›¿ä»£Filter.pyåŠŸèƒ½ï¼‰
        """
        model = self.load_llm_model()
        
        questions = [
            "Question: Does this section cover the types of surface chemical reactions or experimental studies on the formation of molecules on surfaces? Answer 'Yes' or 'No'. \nAnswer:"
        ]
        
        for idx, row in df.iterrows():
            content = row['content']
            classification = 'No'
            
            for question in questions:
                prompt = f"{content}\n{question}"
                try:
                    response = model.generate(prompt=prompt, max_tokens=10, temp=0.1)
                    if response and response.strip():
                        first_word = response.split()[0].replace('.', '').replace(',', '')
                        if first_word not in ['No', 'Not']:
                            classification = first_word
                            break
                except Exception as e:
                    print(f"Error generating response: {e}")
                    continue
            
            df.loc[idx, 'classification'] = classification
        
        # è¿‡æ»¤æ‰"No"çš„æ®µè½
        condition = (df['classification'] != 'No') & (df['classification'] != 'Not')
        df_filtered = df[condition]
        
        return df_filtered
    
    def create_abstract_conclusion_embeddings(self, df):
        """
        åˆ›å»ºæ‘˜è¦å’Œç»“è®ºä¸“ç”¨çš„åµŒå…¥ï¼ˆæ•´åˆAbstract_Conclusion_Embedding.pyåŠŸèƒ½ï¼‰
        """
        # å®šä¹‰æ‘˜è¦å’Œç»“è®ºç›¸å…³çš„å…³é”®è¯
        abstract_conclusion_keywords = [
            "conclusion", "abstract", "summary", "findings", "results", 
            "synthesis parameters", "reaction conditions", "precursor molecules",
            "substrate", "temperature", "products", "experimental results",
            "key findings", "main results", "final results", "outcome",
            "synthesis", "reaction", "molecular", "surface chemistry",
            "on-surface", "catalytic", "formation", "yield", "selectivity"
        ]
        
        # åˆ›å»ºå‚è€ƒæ–‡æœ¬
        reference_text = " ".join(abstract_conclusion_keywords)
        
        # è®¡ç®—åµŒå…¥
        df['content_embedding'] = df['content'].apply(lambda x: self.embedding_model.encode(x, convert_to_tensor=True))
        reference_embedding = self.embedding_model.encode(reference_text, convert_to_tensor=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        df['similarity'] = df['content_embedding'].apply(
            lambda x: cosine_similarity([x.cpu().numpy()], [reference_embedding.cpu().numpy()])[0][0]
        )
        
        return df
    
    def select_top_paragraphs(self, df, top_n=10):
        """
        é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½
        """
        df_sorted = df.sort_values('similarity', ascending=False)
        return df_sorted.head(top_n)
    
    def abstract_text_with_llm(self, df):
        """
        ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬æŠ½è±¡ï¼ˆæ•´åˆAbstract.pyåŠŸèƒ½ï¼‰
        """
        model = self.load_llm_model()
        abstract = []
        
        for index, row in df.iterrows():
            content = row['content']
            
            prompt_template = (
                f"{content}"
                f"Answer the question as truthfully as possible using the provided context."
                f"Please summarize the text below, emphasizing the types of reactions featured in the author's scientific experiments on surface reactions."            
            )
            
            try:
                abstract_text = model.generate(prompt=prompt_template, max_tokens=250, temp=0.0, top_p=0.6)
                print(f"Abstract {index+1}/{len(df)}:")
                print(abstract_text)
                abstract.append(abstract_text)
            except Exception as e:
                print(f"Error generating abstract: {e}")
                abstract.append("Error generating abstract")
        
        df['abstract'] = pd.Series(abstract)
        return df
    
    def summarize_parameters_with_llm(self, df):
        """
        ä½¿ç”¨LLMæ€»ç»“å‚æ•°ï¼ˆæ•´åˆSummerized.pyåŠŸèƒ½ï¼‰
        """
        model = self.load_llm_model()
        summarized = []
        
        for index, row in df.iterrows():
            content = row['content']
            prompt_template = (
                f"{content}\n"            
                f"Task: Please summarize the following details in a table: precursor molecules, substrates, annealing/reaction temperature of the molecules, products (i.e., the compound molecules formed in this experiment), and the dimensionality of the product molecules (Simplified numbers plus letters). If no information is provided or you are unsure, use N/A. Please focus on extracting experimental conditions only from the surface chemistry synthesis. The table should have 5 columns: | Precursor | Substrate | Temperature | Products | Dimensions |"
            )
            
            try:
                summarize_text = model.generate(prompt=prompt_template, max_tokens=250, temp=0.0, top_p=0.6)
                print(f"Summarized {index+1}/{len(df)}:")        
                print(summarize_text)
                summarized.append(summarize_text)
            except Exception as e:
                print(f"Error generating summary: {e}")
                summarized.append("Error generating summary")
        
        df['summarized'] = pd.Series(summarized)
        return df
    
    def save_df_to_text(self, df, file_path, content_column='content'):
        """
        ä¿å­˜DataFrameåˆ°æ–‡æœ¬æ–‡ä»¶
        """
        with open(file_path, 'w', encoding='utf-8') as file:
            for index, row in df.iterrows():
                file.write(row[content_column] + '\n\n')
    
    def process_text_file_comprehensive(self, file_path, mode='comprehensive'):
        """
        ç»¼åˆæ–‡æœ¬å¤„ç†å‡½æ•°
        mode: 'filter' - åªè¿‡æ»¤
              'abstract' - åªæŠ½è±¡
              'summarize' - åªæ€»ç»“
              'comprehensive' - å®Œæ•´æµç¨‹
        """
        print(f"ğŸ” å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        print("=" * 50)
        
        # è¯»å–æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        # åˆ†å‰²æ®µè½
        current_segment = []
        segments = []
        
        for line in lines:
            if line.strip():
                current_segment.append(line.strip())
            else:
                if current_segment:
                    segments.append(' '.join(current_segment))
                    current_segment = []
        
        if current_segment:
            segments.append(' '.join(current_segment))
        
        df = pd.DataFrame(segments, columns=['content'])
        print(f"ğŸ“Š åŸå§‹æ®µè½æ•°: {len(df)}")
        
        output_files = {}
        
        if mode in ['filter', 'comprehensive']:
            # 1. LLMå†…å®¹è¿‡æ»¤
            print("\nğŸ¤– æ­¥éª¤1: LLMå†…å®¹è¿‡æ»¤...")
            df_filtered = self.filter_content_with_llm(df)
            print(f"âœ… è¿‡æ»¤åæ®µè½æ•°: {len(df_filtered)}")
            
            # ä¿å­˜è¿‡æ»¤ç»“æœ
            filter_file = file_path.replace('.txt', '_Filtered.txt')
            self.save_df_to_text(df_filtered, filter_file)
            output_files['filter'] = filter_file
        
        if mode in ['abstract', 'comprehensive']:
            # 2. æ–‡æœ¬æŠ½è±¡
            print("\nğŸ“ æ­¥éª¤2: æ–‡æœ¬æŠ½è±¡...")
            df_abstract = self.abstract_text_with_llm(df_filtered if 'df_filtered' in locals() else df)
            
            # ä¿å­˜æŠ½è±¡ç»“æœ
            abstract_file = file_path.replace('.txt', '_Abstract.txt')
            self.save_df_to_text(df_abstract, abstract_file, 'abstract')
            output_files['abstract'] = abstract_file
        
        if mode in ['summarize', 'comprehensive']:
            # 3. å‚æ•°æ€»ç»“
            print("\nğŸ“Š æ­¥éª¤3: å‚æ•°æ€»ç»“...")
            df_summarized = self.summarize_parameters_with_llm(df_filtered if 'df_filtered' in locals() else df)
            
            # ä¿å­˜æ€»ç»“ç»“æœ
            summarize_file = file_path.replace('.txt', '_Summarized.txt')
            self.save_df_to_text(df_summarized, summarize_file, 'summarized')
            output_files['summarized'] = summarize_file
        
        return output_files

# å…¼å®¹æ€§å‡½æ•°
def process_text_file_for_filter(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='filter')
    return list(result.values())[0]

def process_text_file_for_abstract(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='abstract')
    return list(result.values())[0]

def process_text_file_for_summerized(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='summarize')
    return list(result.values())[0]

# ä½¿ç”¨Meta-Llamaæ¨¡å‹çš„ä¸“ç”¨å‡½æ•°
def process_text_file_for_filter_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='filter')
    return list(result.values())[0]

def process_text_file_for_abstract_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='abstract')
    return list(result.values())[0]

def process_text_file_for_summerized_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='summarize')
    return list(result.values())[0]
