from gpt4all import GPT4All
import pandas as pd
import os
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class UnifiedTextProcessor:
    """
    ç»Ÿä¸€çš„æ–‡æœ¬å¤„ç†ç±»ï¼Œæ•´åˆæ‰€æœ‰æ–‡æœ¬å¤„ç†åŠŸèƒ½
    """
    
    def __init__(self, model_name='nous-hermes-llama2-13b.Q4_0.gguf', model_path='models/'):
        self.model_name = model_name
        self.model_path = model_path
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.model = self.load_llm_model() # åœ¨åˆå§‹åŒ–æ—¶åªåŠ è½½ä¸€æ¬¡
        # è§’è‰²æ‰®æ¼”ï¼Œç³»ç»ŸæŒ‡ä»¤æ‰®æ¼”ä¸“å®¶åŠ©ç†è§’è‰²
        self.system_prompt = (
            "You are an expert assistant for scientific literature mining. "
            "Your task is to follow the user's instructions precisely to extract structured data from scientific texts."
        )
        
    def _create_prompt(self, user_prompt, context=""):
        """
        ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºåˆ›å»ºå¸¦æœ‰ç³»ç»ŸæŒ‡ä»¤çš„å®Œæ•´Promptã€‚
        """
        # ä½¿ç”¨åˆ†éš”ç¬¦è®©ç»“æ„æ›´æ¸…æ™°
        return (
            f"{self.system_prompt}\n\n"
            f"### Paragraph to Analyze ###\n"
            f"{context}\n\n"
            f"### Task ###\n"
            f"{user_prompt}"
        )
    def load_llm_model(self):
        """
        åŠ è½½LLMæ¨¡å‹ï¼Œæ”¯æŒå›é€€æœºåˆ¶
        """
        # è·å–ç»å¯¹è·¯å¾„
        abs_model_path = os.path.abspath(self.model_path)
        print(f"ğŸ” å°è¯•åŠ è½½æ¨¡å‹ï¼Œè·¯å¾„: {abs_model_path}")
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æŒ‡å®šæ¨¡å‹
            model = GPT4All(self.model_name, model_path=abs_model_path, allow_download=False)
            print(f"âœ… æˆåŠŸåŠ è½½ {self.model_name} æ¨¡å‹")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½ {self.model_name} å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„...")
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
                model = GPT4All(self.model_name, allow_download=False)
                print(f"âœ… æˆåŠŸåŠ è½½ {self.model_name} æ¨¡å‹ (é»˜è®¤è·¯å¾„)")
                return model
            except Exception as e2:
                print(f"âŒ é»˜è®¤è·¯å¾„ä¹Ÿå¤±è´¥: {e2}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨Meta-Llama-3.1-8Bæ¨¡å‹...")
                try:
                    # å°è¯•Meta-Llamaæ¨¡å‹
                    model = GPT4All('Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', model_path=abs_model_path, allow_download=False)
                    print("âœ… æˆåŠŸåŠ è½½ Meta-Llama-3.1-8B æ¨¡å‹ (å¤‡é€‰)")
                    return model
                except Exception as e3:
                    print(f"âŒ Meta-Llamaæ¨¡å‹ä¹Ÿå¤±è´¥: {e3}")
                    print("ğŸ”„ æœ€åå›é€€åˆ° nous-hermes-llama2-13b æ¨¡å‹...")
                    try:
                        # æœ€åå›é€€åˆ°nous-hermesæ¨¡å‹
                        model = GPT4All('nous-hermes-llama2-13b.Q4_0.gguf', model_path=abs_model_path, allow_download=False)
                        print("âœ… æˆåŠŸåŠ è½½ nous-hermes-llama2-13b æ¨¡å‹ (æœ€ç»ˆå›é€€)")
                        return model
                    except Exception as e4:
                        print(f"âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å°è¯•éƒ½å¤±è´¥: {e4}")
                        raise e4
    
    # def filter_content_with_llm(self, df):
    #     """
    #     ä½¿ç”¨LLMè¿‡æ»¤å†…å®¹ï¼ˆæ›¿ä»£Filter.pyåŠŸèƒ½ï¼‰
    #     """
    #     # model = self.load_llm_model()
        
    #     ## åŸFilter.pyåŠŸèƒ½
    #     # questions = [
    #     #     "Question: Does this section cover the types of surface chemical reactions or experimental studies on the formation of molecules on surfaces? Answer 'Yes' or 'No'. \nAnswer:"
    #     # ]
    #     questions = [
    #     ("Question: Is this paragraph about flow chemistry or process development, including "
    #      "continuous flow setup, reactor type/ID, flow rates, residence time, temperature, reactant, "
    #      "catalyst, optimization, conversion/yield/selectivity? Answer 'Yes' or 'No'.\nAnswer:")
    #     ]
        
    #     for idx, row in df.iterrows():
    #         content = row['content']
    #         classification = 'No'
            
    #         for question in questions:
    #             prompt = f"{content}\n{question}"
    #             try:
    #                 response = self.model.generate(prompt=prompt, max_tokens=10, temp=0.1)
    #                 if response and response.strip():
    #                     first_word = response.split()[0].replace('.', '').replace(',', '')
    #                     if first_word not in ['No', 'Not']:
    #                         classification = first_word
    #                         break
    #             except Exception as e:
    #                 print(f"Error generating response: {e}")
    #                 continue
            
    #         df.loc[idx, 'classification'] = classification
        
    #     # è¿‡æ»¤æ‰"No"çš„æ®µè½
    #     condition = (df['classification'] != 'No') & (df['classification'] != 'Not')
    #     df_filtered = df[condition]
        
    #     return df_filtered

    def filter_content_with_llm(self, df):
        """
        ä½¿ç”¨LLMè¿‡æ»¤å†…å®¹ï¼Œå·²ä½¿ç”¨æ–°çš„Promptç»“æ„è¿›è¡Œä¼˜åŒ–ã€‚
        """
        # 1. å°†æ ¸å¿ƒé—®é¢˜å®šä¹‰å¾—æ›´æ¸…æ™°ï¼Œä½œä¸ºç”¨æˆ·æŒ‡ä»¤
        user_question = (
            "Based on the criteria below, does the provided paragraph describe an experimental procedure "
            "for flow chemistry or its process development? Answer strictly with 'Yes' or 'No'.\n\n"
            "Criteria: The paragraph should mention specific experimental details, for example: "
            "continuous flow setup, reactor type/ID, flow rates, residence time, temperature, reactant, "
            "catalyst, optimization, or conversion/yield/selectivity.\n\n"
            "Answer:"
        )

        classifications = []  # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥æ”¶é›†æ‰€æœ‰åˆ†ç±»ç»“æœï¼Œæ¯”é€è¡Œä¿®æ”¹DataFrameæ›´é«˜æ•ˆ
        
        print("...å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œæ®µè½åˆ†ç±»...")
        # 2. éå†DataFrameçš„æ¯ä¸€è¡Œ
        for index, row in df.iterrows():
            content = row['content']
            
            # 3. ä½¿ç”¨æ‚¨çš„è¾…åŠ©å‡½æ•°åˆ›å»ºå®Œæ•´çš„ã€å¸¦æœ‰ä¸Šä¸‹æ–‡å’Œç³»ç»ŸæŒ‡ä»¤çš„Prompt
            # å‡è®¾ self.system_prompt å’Œ self._create_prompt å·²åœ¨ç±»ä¸­å®šä¹‰
            full_prompt = self._create_prompt(user_prompt=user_question, context=content)
            
            try:
                # 4. è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”
                # å°†tempè®¾ä¸º0.0ï¼Œè®©æ¨¡å‹çš„å›ç­”æ›´å…·ç¡®å®šæ€§ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
                response = self.model.generate(prompt=full_prompt, max_tokens=5, temp=0.0)
                
                # 5. å¯¹å“åº”è¿›è¡Œæ›´ç¨³å¥çš„è§£æ
                # .strip() å»é™¤é¦–å°¾ç©ºæ ¼, .lower() è½¬ä¸ºå°å†™, .startswith('yes') åˆ¤æ–­æ˜¯å¦ä»¥'yes'å¼€å¤´
                if response and response.strip().lower().startswith('yes'):
                    classifications.append('Yes')
                else:
                    classifications.append('No')

            except Exception as e:
                print(f"å¤„ç†ç¬¬ {index} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                classifications.append('No')  # å¦‚æœå‡ºé”™ï¼Œé»˜è®¤ä¸º'No'

        # 6. ä¸€æ¬¡æ€§å°†æ‰€æœ‰åˆ†ç±»ç»“æœæ·»åŠ åˆ°DataFrameä¸­
        df['classification'] = classifications
        
        # 7. è¿‡æ»¤æ‰ "No" çš„æ®µè½ï¼Œå¹¶ä½¿ç”¨ .copy() é¿å…æ½œåœ¨çš„è­¦å‘Š
        df_filtered = df[df['classification'] == 'Yes'].copy()
        
        print(f"...åˆ†ç±»å®Œæˆï¼Œä¿ç•™ {len(df_filtered)} ä¸ªç›¸å…³æ®µè½ã€‚")
        return df_filtered
    
    def create_abstract_conclusion_embeddings(self, df):
        """
        åˆ›å»ºæ‘˜è¦å’Œç»“è®ºä¸“ç”¨çš„åµŒå…¥ï¼ˆæ•´åˆAbstract_Conclusion_Embedding.pyåŠŸèƒ½ï¼‰
        """
        # å®šä¹‰æ‘˜è¦å’Œç»“è®ºç›¸å…³çš„å…³é”®è¯
        abstract_conclusion_keywords = [
            "conclusion", "abstract", "summary", "findings", "results", 
            "flow chemistry", "continuous flow", "process development", "reactor",
            "flow rate", "residence time", "optimization", "scale-up", "yield",
            "conversion", "selectivity", "catalyst", "temperature", "pressure"
        ]
        
        # åˆ›å»ºå‚è€ƒæ–‡æœ¬
        reference_text = " ".join(abstract_conclusion_keywords)
        
        # è®¡ç®—åµŒå…¥
        df['content_embedding'] = df['content'].apply(lambda x: self.embedding_model.encode(x, convert_to_tensor=True))
        reference_embedding = self.embedding_model.encode(reference_text, convert_to_tensor=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        df['similarity'] = df['content_embedding'].apply(
            lambda x: cosine_similarity([x.cpu().numpy()], [reference_embedding.cpu().numpy()])[0][0]
        )
        
        return df
    
    def select_top_paragraphs(self, df, top_n=10):
        """
        é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½
        """
        df_sorted = df.sort_values('similarity', ascending=False)
        return df_sorted.head(top_n)
    
    def abstract_text_with_llm(self, df):
        """
        ä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬æŠ½è±¡ï¼ˆå·²ä¼˜åŒ–ï¼‰
        """
        abstract = []
        
        # 1. å®šä¹‰é’ˆå¯¹æ­¤ä»»åŠ¡çš„ç”¨æˆ·æŒ‡ä»¤
        user_prompt = (
            "Please summarize the paragraph focusing on flow-chemistry process development. "
            "The summary should highlight: reaction type, reactants/catalyst, products, reactor details, "
            "key conditions (like flow rates, residence time, temperature), "
            "and any reported outcomes (conversion/yield/selectivity). Be concise and faithful to the source text."
        )
        
        for index, row in df.iterrows():
            content = row['content']
            
            # 2. ä½¿ç”¨è¾…åŠ©å‡½æ•°æ„å»ºå®Œæ•´çš„Prompt
            full_prompt = self._create_prompt(user_prompt=user_prompt, context=content)
            
            try:
                # 3. ä½¿ç”¨ self.model è¿›è¡Œè°ƒç”¨
                abstract_text = self.model.generate(prompt=full_prompt, max_tokens=250, temp=0.0, top_p=0.6)
                print(f"Abstract {index+1}/{len(df)}:")
                print(abstract_text)
                abstract.append(abstract_text)
            except Exception as e:
                print(f"Error generating abstract for row {index}: {e}")
                abstract.append(f"Error: {e}")
        
        df['abstract'] = pd.Series(abstract, index=df.index) # ç¡®ä¿ç´¢å¼•å¯¹é½
        return df
    
    def summarize_parameters_with_llm(self, df):
        """
        ä½¿ç”¨LLMæ€»ç»“å‚æ•°ï¼ˆå·²ä¼˜åŒ–ï¼‰
        """
        summarized = []
        
        # 1. å®šä¹‰ä¸€ä¸ªæ¸…æ™°çš„ç”¨æˆ·æŒ‡ä»¤ï¼ŒåŒ…å«æ‰€æœ‰è§„åˆ™å’ŒSchema
        user_prompt = (
            "Extract structured data for flow-chemistry process development as a strict JSON object. "
            # æ²¡æœ‰å°±è¿”å›null é¿å…å¹»è§‰
            "If a field is not explicitly stated, use null. Use original units when present; "
            # åœ¨JSONé‡Œï¼Œåƒè½¬åŒ–ç‡ã€äº§ç‡è¿™äº›æ•°å€¼ï¼Œè¯·ç›´æ¥ç”¨æ•°å­—æ ¼å¼
            "otherwise normalize as: temperature in Â°C, residence_time in min, flow_rate in mL/min, "
            "inner_diameter in mm. Use strings for values with units (e.g., \"100 Â°C\", \"0.20 mL/min\").\n\n"
            "### JSON Schema ###\n"
            "{\n"
            "  \"reaction_summary\": {\n"
            "    \"reaction_type\": \"...\", \n"
            "    \"reactants\": [ {\"name\": \"...\", \"role\": \"reactant|catalyst|solvent\"}, ... ],\n"
            "    \"products\": [ {\"name\": \"...\", \"yield_optimal\": 95, \"unit\": \"%\"}, ... ],\n"
            "    \"conditions\": [\n"
            "      {\"type\": \"temperature\", \"value\": \"...\"},\n"
            "      {\"type\": \"residence_time\", \"value\": \"...\"},\n"
            "      {\"type\": \"flow_rate_reactant_A\", \"value\": \"...\"},\n"
            "      {\"type\": \"flow_rate_total\", \"value\": \"...\"},\n"
            "      {\"type\": \"pressure\", \"value\": \"...\"}\n"
            "    ],\n"
            "    \"reactor\": {\"type\": \"...\", \"inner_diameter\": \"...\"},\n"
            "    \"metrics\": {\"conversion\": ..., \"yield\": ..., \"selectivity\": ..., \"unit\": \"%\"}\n"
            "  }\n"
            "}\n\n"
            "### Rules ###\n"
            # åªè¦çº¯å‡€çš„json ä¸è¦ä»»ä½•å¤šä½™æ–‡å­—
            "- Output ONLY the valid JSON object and nothing else (no introductory text or explanations).\n"
            "- Keep numbers as numbers where possible (e.g., in 'metrics'), but keep units within string values for 'conditions'.\n"
            # åªä½¿ç”¨æä¾›çš„æ®µè½ä½œä¸ºè¯æ®ï¼Œä¸è¦ä»å…¶ä»–éƒ¨åˆ†æ¨æ–­ï¼Œé˜²æ­¢ç‰›å¤´é©¬é¢ ä¹±æ‹¼
            "- Only use the provided paragraph as evidence; do not infer from other parts of the paper.\n"
            # åªæœ‰æœ€ä¼˜é€‰æœ€ä¼˜
            "- Set 'is_optimal': true only if words like 'optimal', 'optimized', 'best' are explicitly present in this paragraph; otherwise null.\n"
            # æ²¡æœ‰æœ€ä¼˜é€‰æœ€é«˜äº§ç‡
            "- If multiple experimental conditions are reported, prioritize the one explicitly labeled as 'optimal'. If none are labeled, select the condition set that corresponds to the best reported performance (e.g., highest yield or conversion).\n"
            "- If multiple reactant streams have distinct flow rates, use specific keys like 'flow_rate_reactant_A', 'flow_rate_reactant_B', and include 'flow_rate_total' if it is also reported.\n"
        )

        for index, row in df.iterrows():
            content = row['content']
            
            # 2. ä½¿ç”¨è¾…åŠ©å‡½æ•°æ„å»ºå®Œæ•´çš„Prompt
            full_prompt = self._create_prompt(user_prompt=user_prompt, context=content)
            
            try:
                # 3. ä½¿ç”¨ self.model è¿›è¡Œè°ƒç”¨
                # å¢åŠ max_tokensä»¥å®¹çº³æ›´å¤æ‚çš„JSONè¾“å‡º
                summarize_text = self.model.generate(prompt=full_prompt, max_tokens=512, temp=0.0, top_p=0.6)
                print(f"Summarized {index+1}/{len(df)}:")        
                print(summarize_text)
                summarized.append(summarize_text)
            except Exception as e:
                print(f"Error generating summary for row {index}: {e}")
                summarized.append(f"Error: {e}")
        
        df['summarized'] = pd.Series(summarized, index=df.index) # ç¡®ä¿ç´¢å¼•å¯¹é½
        return df
    
    def save_df_to_text(self, df, file_path, content_column='content'):
        """
        ä¿å­˜DataFrameåˆ°æ–‡æœ¬æ–‡ä»¶
        """
        with open(file_path, 'w', encoding='utf-8') as file:
            for index, row in df.iterrows():
                file.write(row[content_column] + '\n\n')
    
    def process_text_file_comprehensive(self, file_path, mode='comprehensive'):
        """
        ç»¼åˆæ–‡æœ¬å¤„ç†å‡½æ•°
        mode: 'filter' - åªè¿‡æ»¤
              'abstract' - åªæŠ½è±¡
              'summarize' - åªæ€»ç»“
              'comprehensive' - å®Œæ•´æµç¨‹
        """
        print(f"ğŸ” å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        print("=" * 50)
        
        # è¯»å–æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        # åˆ†å‰²æ®µè½
        current_segment = []
        segments = []
        
        for line in lines:
            if line.strip():
                current_segment.append(line.strip())
            else:
                if current_segment:
                    segments.append(' '.join(current_segment))
                    current_segment = []
        
        if current_segment:
            segments.append(' '.join(current_segment))
        
        df = pd.DataFrame(segments, columns=['content'])
        print(f"ğŸ“Š åŸå§‹æ®µè½æ•°: {len(df)}")
        
        output_files = {}
        
        if mode in ['filter', 'comprehensive']:
            # 1. LLMå†…å®¹è¿‡æ»¤
            print("\nğŸ¤– æ­¥éª¤1: LLMå†…å®¹è¿‡æ»¤...")
            df_filtered = self.filter_content_with_llm(df)
            print(f"âœ… è¿‡æ»¤åæ®µè½æ•°: {len(df_filtered)}")
            
            # ä¿å­˜è¿‡æ»¤ç»“æœ
            filter_file = file_path.replace('.txt', '_Filtered.txt')
            self.save_df_to_text(df_filtered, filter_file)
            output_files['filter'] = filter_file
        
        if mode in ['abstract', 'comprehensive']:
            # 2. æ–‡æœ¬æŠ½è±¡
            print("\nğŸ“ æ­¥éª¤2: æ–‡æœ¬æŠ½è±¡...")
            df_abstract = self.abstract_text_with_llm(df_filtered if 'df_filtered' in locals() else df)
            
            # ä¿å­˜æŠ½è±¡ç»“æœ
            abstract_file = file_path.replace('.txt', '_Abstract.txt')
            self.save_df_to_text(df_abstract, abstract_file, 'abstract')
            output_files['abstract'] = abstract_file
        
        if mode in ['summarize', 'comprehensive']:
            # 3. å‚æ•°æ€»ç»“
            print("\nğŸ“Š æ­¥éª¤3: å‚æ•°æ€»ç»“...")
            df_summarized = self.summarize_parameters_with_llm(df_filtered if 'df_filtered' in locals() else df)
            
            # ä¿å­˜æ€»ç»“ç»“æœ
            summarize_file = file_path.replace('.txt', '_Summarized.txt')
            self.save_df_to_text(df_summarized, summarize_file, 'summarized')
            output_files['summarized'] = summarize_file
        
        return output_files

# å…¼å®¹æ€§å‡½æ•°
def process_text_file_for_filter(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='filter')
    return list(result.values())[0]

def process_text_file_for_abstract(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='abstract')
    return list(result.values())[0]

def process_text_file_for_summerized(file_path):
    processor = UnifiedTextProcessor()
    result = processor.process_text_file_comprehensive(file_path, mode='summarize')
    return list(result.values())[0]

# ä½¿ç”¨Meta-Llamaæ¨¡å‹çš„ä¸“ç”¨å‡½æ•°
def process_text_file_for_filter_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='filter')
    return list(result.values())[0]

def process_text_file_for_abstract_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='abstract')
    return list(result.values())[0]

def process_text_file_for_summerized_meta_llama(file_path):
    processor = UnifiedTextProcessor(model_name='Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf')
    result = processor.process_text_file_comprehensive(file_path, mode='summarize')
    return list(result.values())[0]
