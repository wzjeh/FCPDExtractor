================================================================================
📖 FCPDExtractor 模型配置使用指南
================================================================================

✨ 新功能：分阶段自定义LLM模型

现在你可以为不同的处理步骤选择不同的LLM模型，实现更灵活的提取策略！

================================================================================
🚀 快速开始（3步搞定）
================================================================================

第1步：打开 OSSExtractor_Debug.ipynb，找到 Cell 7（模型配置单元格）

第2步：根据需要修改模型配置，例如：

    MODEL_FILTER = 'nous-hermes-llama2-13b.Q4_0.gguf'        # 步骤4用的模型
    MODEL_ABSTRACT = 'nous-hermes-llama2-13b.Q4_0.gguf'      # 步骤5抽象用的模型
    MODEL_SUMMARIZE = 'meta-llama-3.1-8b-instruct-q4_k_m-2.gguf'  # 步骤5总结用的模型
    STRICT_MODE = True                                        # 是否严格使用指定模型
    TOP_N_PARAGRAPHS = 10                                    # 相似度筛选保留的段落数

第3步：依次运行：
    - Cell 7（应用配置）
    - Cell 27（重载模块）← 重要！
    - 继续运行步骤4、5

================================================================================
📦 可用模型（需放在 models/ 目录下）
================================================================================

模型名称                                     大小      特点              推荐用途
────────────────────────────────────────────────────────────────────────────
nous-hermes-llama2-13b.Q4_0.gguf            ~7GB     稳定快速          默认全流程
gpt-oss-20b-Q4_K_M.gguf                     ~12GB    容量大理解强      复杂总结
meta-llama-3.1-8b-instruct-q4_k_m-2.gguf    ~5GB     指令遵循好        参数提取
mistral-7b-instruct-v0.1.Q4_0.gguf          ~4GB     轻量快速          快速过滤/抽象
phi-3-mini-4k-instruct.Q4_0.gguf            ~2GB     超轻量            简单过滤

================================================================================
💡 推荐配置方案
================================================================================

【方案A：速度优先】适合快速测试或大批量处理
    MODEL_FILTER = 'nous-hermes-llama2-13b.Q4_0.gguf'
    MODEL_ABSTRACT = 'nous-hermes-llama2-13b.Q4_0.gguf'
    MODEL_SUMMARIZE = 'nous-hermes-llama2-13b.Q4_0.gguf'
    TOP_N_PARAGRAPHS = 10

【方案B：质量优先】适合重要文献的精细提取
    MODEL_FILTER = 'nous-hermes-llama2-13b.Q4_0.gguf'
    MODEL_ABSTRACT = 'mistral-7b-instruct-v0.1.Q4_0.gguf'
    MODEL_SUMMARIZE = 'gpt-oss-20b-Q4_K_M.gguf'
    STRICT_MODE = True
    TOP_N_PARAGRAPHS = 20

【方案C：平衡模式】推荐！速度和质量平衡
    MODEL_FILTER = 'nous-hermes-llama2-13b.Q4_0.gguf'
    MODEL_ABSTRACT = 'nous-hermes-llama2-13b.Q4_0.gguf'
    MODEL_SUMMARIZE = 'meta-llama-3.1-8b-instruct-q4_k_m-2.gguf'
    STRICT_MODE = True
    TOP_N_PARAGRAPHS = 10

================================================================================
⚙️ 参数说明
================================================================================

STRICT_MODE：
  • True：严格使用指定模型，加载失败则报错（推荐生产环境）
  • False：加载失败时自动回退到备用模型（适合测试）

TOP_N_PARAGRAPHS：
  • 10（默认）：适合短篇论文或快速提取
  • 20-30：适合长篇综述或需要更全面的信息
  • 5：适合非常聚焦的快速筛选

================================================================================
❗ 重要提示
================================================================================

1. 修改配置后必须运行 Cell 27 重载模块，否则不生效！
2. 确保指定的 GGUF 文件在 models/ 目录下
3. 大模型需要更多内存，注意系统资源
4. 如需重新处理，先删除之前的中间文件

================================================================================
🔧 常见问题
================================================================================

问：修改配置后不生效？
答：1) 重新运行 Cell 7
    2) 必须运行 Cell 27 重载模块
    3) 或者重启 Jupyter Kernel

问：模型加载失败？
答：1) 检查 models/ 目录下是否有该文件
    2) 确认文件名拼写正确（区分大小写）
    3) 确保文件下载完整（检查文件大小）

问：内存不足？
答：1) 换用更小的模型（如 Phi-3 或 Mistral-7B）
    2) 减少 TOP_N_PARAGRAPHS 参数
    3) 关闭其他占用内存的应用

================================================================================
📚 详细文档
================================================================================

完整的英文文档请参考：MODEL_CONFIG_GUIDE.md
更新说明请参考：CHANGES_SUMMARY.md

================================================================================
